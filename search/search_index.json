{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pipen - A pipeline framework for python Installation pip install -U pipen Quickstart example.py from pipen import Proc , Pipen class Subset ( Proc ): \"\"\"Subset the input data using pandas\"\"\" input_keys = 'datafile' input = [ 'https://raw.githubusercontent.com/tidyverse/ggplot2/master/data-raw/mpg.csv' ] output = 'outfile:file:mpg-subset.csv' lang = 'python' script = \"\"\" import pandas data = pandas.read_csv('{{in.datafile}}') data = data[['model', 'displ']] data.to_csv('{{out.outfile}}') \"\"\" class Plot ( Proc ): \"\"\"Plot the data with ggplot2 in R\"\"\" requires = Subset input_keys = 'datafile:file' output = 'plotfile:file:mpg.png' lang = 'Rscript' script = \"\"\" library(ggplot2) data = read.csv('{{in.datafile}}') png('{{out.plotfile}}') ggplot(data) + geom_boxplot(aes(x=model, y=displ)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) dev.off() \"\"\" if __name__ == '__main__' : pipen = Pipen ( name = 'plot-mpg' , starts = Subset ) pipen . run () $ python example.py","title":"Introduction"},{"location":"#pipen-a-pipeline-framework-for-python","text":"","title":"pipen - A pipeline framework for python"},{"location":"#installation","text":"pip install -U pipen","title":"Installation"},{"location":"#quickstart","text":"example.py from pipen import Proc , Pipen class Subset ( Proc ): \"\"\"Subset the input data using pandas\"\"\" input_keys = 'datafile' input = [ 'https://raw.githubusercontent.com/tidyverse/ggplot2/master/data-raw/mpg.csv' ] output = 'outfile:file:mpg-subset.csv' lang = 'python' script = \"\"\" import pandas data = pandas.read_csv('{{in.datafile}}') data = data[['model', 'displ']] data.to_csv('{{out.outfile}}') \"\"\" class Plot ( Proc ): \"\"\"Plot the data with ggplot2 in R\"\"\" requires = Subset input_keys = 'datafile:file' output = 'plotfile:file:mpg.png' lang = 'Rscript' script = \"\"\" library(ggplot2) data = read.csv('{{in.datafile}}') png('{{out.plotfile}}') ggplot(data) + geom_boxplot(aes(x=model, y=displ)) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) dev.off() \"\"\" if __name__ == '__main__' : pipen = Pipen ( name = 'plot-mpg' , starts = Subset ) pipen . run () $ python example.py","title":"Quickstart"},{"location":"CHANGELOG/","text":"0.0.1 Reimplement PyPPL using asyncio","title":"Change log"},{"location":"CHANGELOG/#001","text":"Reimplement PyPPL using asyncio","title":"0.0.1"},{"location":"api/pipen.channel/","text":"package pipen . channel </> Provide some function for creating and modifying channels(dataframes) Functions create ( value ) (DataFrame) \u2014 Create a channel from a list. </> from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern </> from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </> module pipen.channel . vector </> Vector help functions for pipen channels module pipen.channel . verbs </> Verbs for pipen channels Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in , other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in , other cols will keep the same. </>","title":"pipen.channel"},{"location":"api/pipen.channel/#pipenchannel","text":"</> Provide some function for creating and modifying channels(dataframes) Functions create ( value ) (DataFrame) \u2014 Create a channel from a list. </> from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern </> from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </> module","title":"pipen.channel"},{"location":"api/pipen.channel/#pipenchannelvector","text":"</> Vector help functions for pipen channels module","title":"pipen.channel.vector"},{"location":"api/pipen.channel/#pipenchannelverbs","text":"</> Verbs for pipen channels Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in , other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in , other cols will keep the same. </>","title":"pipen.channel.verbs"},{"location":"api/pipen.channel.vector/","text":"module pipen.channel . vector </> Vector help functions for pipen channels","title":"pipen.channel.vector"},{"location":"api/pipen.channel.vector/#pipenchannelvector","text":"</> Vector help functions for pipen channels","title":"pipen.channel.vector"},{"location":"api/pipen.channel.verbs/","text":"module pipen.channel . verbs </> Verbs for pipen channels Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in , other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in , other cols will keep the same. </> function pipen.channel.verbs . expand_dir ( data , col=0 , pattern='*' , ftype='any' , sortby='name' , reverse=False ) </> Expand a Channel according to the files in , other cols will keep the same. This is only applicable to a 1-row channel. Examples >>> ch = channel . create ([( './' , 1 )]) >>> ch >> expand () >>> [[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]] Parameters col (str or int, optional) \u2014 the index or name of the column used to expand pattern (str, optional) \u2014 use a pattern to filter the files/dirs, default: * ftype (str, optional) \u2014 the type of the files/dirs to include - 'dir', 'file', 'link' or 'any' (default) sortby (str, optional) \u2014 how the list is sorted - 'name' (default), 'mtime', 'size' reverse (bool, optional) \u2014 reverse sort. Returns (DataFrame) The expanded channel function pipen.channel.verbs . collapse_files ( data , col=0 ) </> Collapse a Channel according to the files in , other cols will use the values in row 0. Note that other values in other rows will be discarded. Examples >>> ch = channel . create ([[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]]) >>> ch >> collapse () >>> [[ '.' , 1 ]] Parameters data (DataFrame) \u2014 The original channel col (str or int, optional) \u2014 the index or name of the column used to collapse on Returns (DataFrame) The collapsed channel","title":"pipen.channel.verbs"},{"location":"api/pipen.channel.verbs/#pipenchannelverbs","text":"</> Verbs for pipen channels Functions collapse_files ( data , col ) (DataFrame) \u2014 Collapse a Channel according to the files in , other cols will use the values in row 0. </> expand_dir ( data , col , pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Expand a Channel according to the files in , other cols will keep the same. </> function","title":"pipen.channel.verbs"},{"location":"api/pipen.channel.verbs/#pipenchannelverbsexpand_dir","text":"</> Expand a Channel according to the files in , other cols will keep the same. This is only applicable to a 1-row channel. Examples >>> ch = channel . create ([( './' , 1 )]) >>> ch >> expand () >>> [[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]] Parameters col (str or int, optional) \u2014 the index or name of the column used to expand pattern (str, optional) \u2014 use a pattern to filter the files/dirs, default: * ftype (str, optional) \u2014 the type of the files/dirs to include - 'dir', 'file', 'link' or 'any' (default) sortby (str, optional) \u2014 how the list is sorted - 'name' (default), 'mtime', 'size' reverse (bool, optional) \u2014 reverse sort. Returns (DataFrame) The expanded channel function","title":"pipen.channel.verbs.expand_dir"},{"location":"api/pipen.channel.verbs/#pipenchannelverbscollapse_files","text":"</> Collapse a Channel according to the files in , other cols will use the values in row 0. Note that other values in other rows will be discarded. Examples >>> ch = channel . create ([[ './a' , 1 ], [ './b' , 1 ], [ './c' , 1 ]]) >>> ch >> collapse () >>> [[ '.' , 1 ]] Parameters data (DataFrame) \u2014 The original channel col (str or int, optional) \u2014 the index or name of the column used to collapse on Returns (DataFrame) The collapsed channel","title":"pipen.channel.verbs.collapse_files"},{"location":"api/pipen.cli/","text":"module pipen . cli </> Provide cli for pipen Functions main ( ) \u2014 Main entry point for cli </> function pipen.cli . main ( ) </> Main entry point for cli","title":"pipen.cli"},{"location":"api/pipen.cli/#pipencli","text":"</> Provide cli for pipen Functions main ( ) \u2014 Main entry point for cli </> function","title":"pipen.cli"},{"location":"api/pipen.cli/#pipenclimain","text":"</> Main entry point for cli","title":"pipen.cli.main"},{"location":"api/pipen.defaults/","text":"module pipen . defaults </> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> class pipen.defaults . ProcInputType ( ) </> Types for process inputs class pipen.defaults . ProcOutputType ( ) </> Types for process outputs","title":"pipen.defaults"},{"location":"api/pipen.defaults/#pipendefaults","text":"</> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> class","title":"pipen.defaults"},{"location":"api/pipen.defaults/#pipendefaultsprocinputtype","text":"</> Types for process inputs class","title":"pipen.defaults.ProcInputType"},{"location":"api/pipen.defaults/#pipendefaultsprocoutputtype","text":"</> Types for process outputs","title":"pipen.defaults.ProcOutputType"},{"location":"api/pipen.exceptions/","text":"module pipen . exceptions </> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEnginTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> ConfigurationError \u2014 When something wrong set as configuration </> ProcWorkdirConflictException \u2014 \"When more than one processes are sharing the same workdir </> class pipen.exceptions . PipenException ( ) </> Bases Exception BaseException Base exception class for pipen class pipen.exceptions . ProcInputTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported input type is provided class pipen.exceptions . ProcScriptFileNotFound ( ) </> Bases pipen.exceptions.PipenException FileNotFoundError OSError Exception BaseException When script file specified as 'file://' cannot be found class pipen.exceptions . ProcOutputNameError ( ) </> Bases pipen.exceptions.PipenException NameError Exception BaseException When no name or malformatted output is provided class pipen.exceptions . ProcOutputTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported output type is provided class pipen.exceptions . ProcOutputValueError ( ) </> Bases pipen.exceptions.PipenException ValueError Exception BaseException When a malformatted output value is provided class pipen.exceptions . ProcDependencyError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When there is something wrong the process dependencies class pipen.exceptions . NoSuchSchedulerError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When specified scheduler cannot be found class pipen.exceptions . WrongSchedulerTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified scheduler is not a subclass of Scheduler class pipen.exceptions . NoSuchTemplateEngineError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When specified template engine cannot be found class pipen.exceptions . WrongTemplateEnginTypeError ( ) </> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified tempalte engine is not a subclass of Scheduler class pipen.exceptions . ConfigurationError ( ) </> Bases pipen.exceptions.PipenException Exception BaseException When something wrong set as configuration class pipen.exceptions . ProcWorkdirConflictException ( ) </> Bases pipen.exceptions.PipenException Exception BaseException \"When more than one processes are sharing the same workdir","title":"pipen.exceptions"},{"location":"api/pipen.exceptions/#pipenexceptions","text":"</> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEnginTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> ConfigurationError \u2014 When something wrong set as configuration </> ProcWorkdirConflictException \u2014 \"When more than one processes are sharing the same workdir </> class","title":"pipen.exceptions"},{"location":"api/pipen.exceptions/#pipenexceptionspipenexception","text":"</> Bases Exception BaseException Base exception class for pipen class","title":"pipen.exceptions.PipenException"},{"location":"api/pipen.exceptions/#pipenexceptionsprocinputtypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported input type is provided class","title":"pipen.exceptions.ProcInputTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocscriptfilenotfound","text":"</> Bases pipen.exceptions.PipenException FileNotFoundError OSError Exception BaseException When script file specified as 'file://' cannot be found class","title":"pipen.exceptions.ProcScriptFileNotFound"},{"location":"api/pipen.exceptions/#pipenexceptionsprocoutputnameerror","text":"</> Bases pipen.exceptions.PipenException NameError Exception BaseException When no name or malformatted output is provided class","title":"pipen.exceptions.ProcOutputNameError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocoutputtypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When an unsupported output type is provided class","title":"pipen.exceptions.ProcOutputTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocoutputvalueerror","text":"</> Bases pipen.exceptions.PipenException ValueError Exception BaseException When a malformatted output value is provided class","title":"pipen.exceptions.ProcOutputValueError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocdependencyerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When there is something wrong the process dependencies class","title":"pipen.exceptions.ProcDependencyError"},{"location":"api/pipen.exceptions/#pipenexceptionsnosuchschedulererror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When specified scheduler cannot be found class","title":"pipen.exceptions.NoSuchSchedulerError"},{"location":"api/pipen.exceptions/#pipenexceptionswrongschedulertypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified scheduler is not a subclass of Scheduler class","title":"pipen.exceptions.WrongSchedulerTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionsnosuchtemplateengineerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When specified template engine cannot be found class","title":"pipen.exceptions.NoSuchTemplateEngineError"},{"location":"api/pipen.exceptions/#pipenexceptionswrongtemplateengintypeerror","text":"</> Bases pipen.exceptions.PipenException TypeError Exception BaseException When specified tempalte engine is not a subclass of Scheduler class","title":"pipen.exceptions.WrongTemplateEnginTypeError"},{"location":"api/pipen.exceptions/#pipenexceptionsconfigurationerror","text":"</> Bases pipen.exceptions.PipenException Exception BaseException When something wrong set as configuration class","title":"pipen.exceptions.ConfigurationError"},{"location":"api/pipen.exceptions/#pipenexceptionsprocworkdirconflictexception","text":"</> Bases pipen.exceptions.PipenException Exception BaseException \"When more than one processes are sharing the same workdir","title":"pipen.exceptions.ProcWorkdirConflictException"},{"location":"api/pipen.job/","text":"module pipen . job </> Provide the Job class Classes Job ( *args , **kwargs ) \u2014 The job for pipen </> abstract class pipen.job . Job ( *args , **kwargs ) </> Bases xqute.job.Job abc.ABC pipen._job_caching.JobCaching The job for pipen Attributes cached (bool) \u2014 check if a job is cached </> lock_file (Path) \u2014 The lock file of the job </> rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrap_cmd ( scheduler ) \u2014 Wrap the command for the scheduler to submit and run </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method cache ( ) </> write signature to signature file class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method __repr__ ( ) \u2192 str </> repr of the job method clean ( retry=False ) </> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method wrapped_script ( scheduler ) </> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script abstract method wrap_cmd ( scheduler ) </> Wrap the command for the scheduler to submit and run Parameters scheduler (Scheduler) \u2014 The scheduler method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object","title":"pipen.job"},{"location":"api/pipen.job/#pipenjob","text":"</> Provide the Job class Classes Job ( *args , **kwargs ) \u2014 The job for pipen </> abstract class","title":"pipen.job"},{"location":"api/pipen.job/#pipenjobjob","text":"</> Bases xqute.job.Job abc.ABC pipen._job_caching.JobCaching The job for pipen Attributes cached (bool) \u2014 check if a job is cached </> lock_file (Path) \u2014 The lock file of the job </> rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrap_cmd ( scheduler ) \u2014 Wrap the command for the scheduler to submit and run </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method","title":"pipen.job.Job"},{"location":"api/pipen.job/#pipen_job_cachingjobcachingcache","text":"</> write signature to signature file class","title":"pipen._job_caching.JobCaching.cache"},{"location":"api/pipen.job/#abcabcmeta","text":"</> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method","title":"abc.ABCMeta"},{"location":"api/pipen.job/#xqutejobjobrepr","text":"</> repr of the job method","title":"xqute.job.Job.repr"},{"location":"api/pipen.job/#xqutejobjobclean","text":"</> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method","title":"xqute.job.Job.clean"},{"location":"api/pipen.job/#xqutejobjobwrapped_script","text":"</> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script abstract method","title":"xqute.job.Job.wrapped_script"},{"location":"api/pipen.job/#xqutejobjobwrap_cmd","text":"</> Wrap the command for the scheduler to submit and run Parameters scheduler (Scheduler) \u2014 The scheduler method","title":"xqute.job.Job.wrap_cmd"},{"location":"api/pipen.job/#pipenjobjoblog","text":"</> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method","title":"pipen.job.Job.log"},{"location":"api/pipen.job/#pipenjobjobprepare","text":"</> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object","title":"pipen.job.Job.prepare"},{"location":"api/pipen/","text":"package pipen </> Entrance of the package. module pipen . template </> Template adaptor for pipen Classes Template ( source , **envs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (type of Template ) \u2014 Get the template engine by name or the template engine itself </> module pipen . scheduler </> Provide builting schedulers Classes LocalJob \u2014 Job class for local scheduler </> LocalScheduler \u2014 Local scheduler </> SgeJob \u2014 Job class for sge scheduler </> SgeScheduler \u2014 Sge scheduler </> Functions get_scheduler ( scheduler ) (type of Scheduler) \u2014 Get the scheduler by name of the scheduler class itself </> module pipen . proc </> Provide the Proc class Classes Proc \u2014 The Proc class provides process assembly functionality </> module pipen . plugin </> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin main plugin, used to update the progress bar and cache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen ) \u2014 The the pipeline is complete. </> on_init ( pipen ) \u2014 The the pipeline is initialized. </> on_job_failed ( proc , job ) \u2014 When a job is done but failed. </> on_job_init ( proc , job ) \u2014 When a job is initialized </> on_job_killed ( proc , job ) \u2014 When a job is killed </> on_job_killing ( proc , job ) (bool, optional) \u2014 When a job is being killed. </> on_job_queued ( proc , job ) \u2014 When a job is queued in xqute. Note it might not be queued yet in the scheduler system. </> on_job_running ( proc , job ) \u2014 When a job starts to run in scheduler system. </> on_job_submitted ( proc , job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( proc , job ) (bool, optional) \u2014 When a job is submitting. </> on_job_succeeded ( proc , job ) \u2014 When a job completes successfully. </> on_proc_done ( proc ) \u2014 When a process is done </> on_proc_init ( proc ) \u2014 When a process is initialized </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_setup ( plugin_config ) \u2014 Setup for plugins, primarily used for the plugins to setup some default configurations </> module pipen . defaults </> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> module pipen . cli </> Provide cli for pipen Functions main ( ) \u2014 Main entry point for cli </> module pipen . pipen </> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> module pipen . progressbar </> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> module pipen . utils </> Provide some utilities Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> get_console_width ( default , shift ) (int) \u2014 Get the console width </> get_logger ( name , level ) (Logger) \u2014 Get the logger by given plugin name </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path. </> get_plugin_context ( plugins ) (SimplugContext) \u2014 Get the plugin context to enable and disable plugins per pipeline </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of cls </> load_entrypoints ( group ) (iterable of (str, any)) \u2014 Load objects from setuptools entrypoints by given group name </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> pipen_banner ( ) (ConsoleRenderable, RichCast, or str) \u2014 The banner for pipen </> render_scope ( scope , title ) (ConsoleRenderable, RichCast, or str) \u2014 Log a mapping to console </> module pipen . exceptions </> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEnginTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> ConfigurationError \u2014 When something wrong set as configuration </> ProcWorkdirConflictException \u2014 \"When more than one processes are sharing the same workdir </> module pipen . job </> Provide the Job class Classes Job ( *args , **kwargs ) \u2014 The job for pipen </> package pipen . channel </> Provide some function for creating and modifying channels(dataframes) Functions create ( value ) (DataFrame) \u2014 Create a channel from a list. </> from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern </> from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </>","title":"pipen"},{"location":"api/pipen/#pipen","text":"</> Entrance of the package. module","title":"pipen"},{"location":"api/pipen/#pipentemplate","text":"</> Template adaptor for pipen Classes Template ( source , **envs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (type of Template ) \u2014 Get the template engine by name or the template engine itself </> module","title":"pipen.template"},{"location":"api/pipen/#pipenscheduler","text":"</> Provide builting schedulers Classes LocalJob \u2014 Job class for local scheduler </> LocalScheduler \u2014 Local scheduler </> SgeJob \u2014 Job class for sge scheduler </> SgeScheduler \u2014 Sge scheduler </> Functions get_scheduler ( scheduler ) (type of Scheduler) \u2014 Get the scheduler by name of the scheduler class itself </> module","title":"pipen.scheduler"},{"location":"api/pipen/#pipenproc","text":"</> Provide the Proc class Classes Proc \u2014 The Proc class provides process assembly functionality </> module","title":"pipen.proc"},{"location":"api/pipen/#pipenplugin","text":"</> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin main plugin, used to update the progress bar and cache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen ) \u2014 The the pipeline is complete. </> on_init ( pipen ) \u2014 The the pipeline is initialized. </> on_job_failed ( proc , job ) \u2014 When a job is done but failed. </> on_job_init ( proc , job ) \u2014 When a job is initialized </> on_job_killed ( proc , job ) \u2014 When a job is killed </> on_job_killing ( proc , job ) (bool, optional) \u2014 When a job is being killed. </> on_job_queued ( proc , job ) \u2014 When a job is queued in xqute. Note it might not be queued yet in the scheduler system. </> on_job_running ( proc , job ) \u2014 When a job starts to run in scheduler system. </> on_job_submitted ( proc , job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( proc , job ) (bool, optional) \u2014 When a job is submitting. </> on_job_succeeded ( proc , job ) \u2014 When a job completes successfully. </> on_proc_done ( proc ) \u2014 When a process is done </> on_proc_init ( proc ) \u2014 When a process is initialized </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_setup ( plugin_config ) \u2014 Setup for plugins, primarily used for the plugins to setup some default configurations </> module","title":"pipen.plugin"},{"location":"api/pipen/#pipendefaults","text":"</> Provide some default values/objects Classes ProcInputType \u2014 Types for process inputs </> ProcOutputType \u2014 Types for process outputs </> module","title":"pipen.defaults"},{"location":"api/pipen/#pipencli","text":"</> Provide cli for pipen Functions main ( ) \u2014 Main entry point for cli </> module","title":"pipen.cli"},{"location":"api/pipen/#pipenpipen","text":"</> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> module","title":"pipen.pipen"},{"location":"api/pipen/#pipenprogressbar","text":"</> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> module","title":"pipen.progressbar"},{"location":"api/pipen/#pipenutils","text":"</> Provide some utilities Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> get_console_width ( default , shift ) (int) \u2014 Get the console width </> get_logger ( name , level ) (Logger) \u2014 Get the logger by given plugin name </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path. </> get_plugin_context ( plugins ) (SimplugContext) \u2014 Get the plugin context to enable and disable plugins per pipeline </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of cls </> load_entrypoints ( group ) (iterable of (str, any)) \u2014 Load objects from setuptools entrypoints by given group name </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> pipen_banner ( ) (ConsoleRenderable, RichCast, or str) \u2014 The banner for pipen </> render_scope ( scope , title ) (ConsoleRenderable, RichCast, or str) \u2014 Log a mapping to console </> module","title":"pipen.utils"},{"location":"api/pipen/#pipenexceptions","text":"</> Provide exception classes Classes PipenException \u2014 Base exception class for pipen </> ProcInputTypeError \u2014 When an unsupported input type is provided </> ProcScriptFileNotFound \u2014 When script file specified as 'file://' cannot be found </> ProcOutputNameError \u2014 When no name or malformatted output is provided </> ProcOutputTypeError \u2014 When an unsupported output type is provided </> ProcOutputValueError \u2014 When a malformatted output value is provided </> ProcDependencyError \u2014 When there is something wrong the process dependencies </> NoSuchSchedulerError \u2014 When specified scheduler cannot be found </> WrongSchedulerTypeError \u2014 When specified scheduler is not a subclass of Scheduler </> NoSuchTemplateEngineError \u2014 When specified template engine cannot be found </> WrongTemplateEnginTypeError \u2014 When specified tempalte engine is not a subclass of Scheduler </> ConfigurationError \u2014 When something wrong set as configuration </> ProcWorkdirConflictException \u2014 \"When more than one processes are sharing the same workdir </> module","title":"pipen.exceptions"},{"location":"api/pipen/#pipenjob","text":"</> Provide the Job class Classes Job ( *args , **kwargs ) \u2014 The job for pipen </> package","title":"pipen.job"},{"location":"api/pipen/#pipenchannel","text":"</> Provide some function for creating and modifying channels(dataframes) Functions create ( value ) (DataFrame) \u2014 Create a channel from a list. </> from_glob ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a channel with a glob pattern </> from_pairs ( pattern , ftype , sortby , reverse ) (DataFrame) \u2014 Create a width=2 channel with a glob pattern </>","title":"pipen.channel"},{"location":"api/pipen.pipen/","text":"module pipen . pipen </> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> class pipen.pipen . Pipen ( starts , name=None , desc='Undescribed.' , outdir=None , **kwargs ) </> The Pipen class provides interface to assemble and run the pipeline Methods async_run ( ) \u2014 Run the processes one by one </> run ( profile ) \u2014 Run the pipeline with the given profile </> method async_run ( ) </> Run the processes one by one method run ( profile='default' ) </> Run the pipeline with the given profile Parameters profile (str, optional) \u2014 The default profile to use for the run Unless the profile is defined in the processes, otherwise this profile will be used","title":"pipen.pipen"},{"location":"api/pipen.pipen/#pipenpipen","text":"</> Main entry module, provide the Pipen class Classes Pipen \u2014 The Pipen class provides interface to assemble and run the pipeline </> class","title":"pipen.pipen"},{"location":"api/pipen.pipen/#pipenpipenpipen","text":"</> The Pipen class provides interface to assemble and run the pipeline Methods async_run ( ) \u2014 Run the processes one by one </> run ( profile ) \u2014 Run the pipeline with the given profile </> method","title":"pipen.pipen.Pipen"},{"location":"api/pipen.pipen/#pipenpipenpipenasync_run","text":"</> Run the processes one by one method","title":"pipen.pipen.Pipen.async_run"},{"location":"api/pipen.pipen/#pipenpipenpipenrun","text":"</> Run the pipeline with the given profile Parameters profile (str, optional) \u2014 The default profile to use for the run Unless the profile is defined in the processes, otherwise this profile will be used","title":"pipen.pipen.Pipen.run"},{"location":"api/pipen.plugin/","text":"module pipen . plugin </> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin main plugin, used to update the progress bar and cache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen ) \u2014 The the pipeline is complete. </> on_init ( pipen ) \u2014 The the pipeline is initialized. </> on_job_failed ( proc , job ) \u2014 When a job is done but failed. </> on_job_init ( proc , job ) \u2014 When a job is initialized </> on_job_killed ( proc , job ) \u2014 When a job is killed </> on_job_killing ( proc , job ) (bool, optional) \u2014 When a job is being killed. </> on_job_queued ( proc , job ) \u2014 When a job is queued in xqute. Note it might not be queued yet in the scheduler system. </> on_job_running ( proc , job ) \u2014 When a job starts to run in scheduler system. </> on_job_submitted ( proc , job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( proc , job ) (bool, optional) \u2014 When a job is submitting. </> on_job_succeeded ( proc , job ) \u2014 When a job completes successfully. </> on_proc_done ( proc ) \u2014 When a process is done </> on_proc_init ( proc ) \u2014 When a process is initialized </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_setup ( plugin_config ) \u2014 Setup for plugins, primarily used for the plugins to setup some default configurations </> function pipen.plugin . on_setup ( plugin_config ) </> Setup for plugins, primarily used for the plugins to setup some default configurations Parameters plugin_config (dict(str: any)) \u2014 The plugin configuration dictionary One should define a configuration item either with a prefix as the identity for the plugin or a namespace inside the plugin config. function pipen.plugin . on_init ( pipen ) </> The the pipeline is initialized. Parameters pipen (Pipen) \u2014 The Pipen object function pipen.plugin . on_complete ( pipen ) </> The the pipeline is complete. Note that this hook is only called when the pipeline is successfully completed Parameters pipen (Pipen) \u2014 The Pipen object function pipen.plugin . on_proc_init ( proc ) </> When a process is initialized Parameters proc (Proc) \u2014 The process function pipen.plugin . on_proc_shutdown ( proc , sig ) </> When pipeline is shutting down, by Ctrl-c for example. Return False to stop shutting down, but you have to shut it down by yourself, for example, proc.xqute.task.cancel() Only the first return value will be used. Parameters sig (Signals, optional) \u2014 The signal. None means a natural shutdown pipen \u2014 The xqute object function pipen.plugin . on_proc_done ( proc ) </> When a process is done This hook will be called anyway when a proc is succeeded or failed. To check if the process is succeeded, use proc.succeeded Parameters proc (Proc) \u2014 The process function pipen.plugin . on_job_init ( proc , job ) </> When a job is initialized Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function pipen.plugin . on_job_queued ( proc , job ) </> When a job is queued in xqute. Note it might not be queued yet in the scheduler system. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function pipen.plugin . on_job_submitting ( proc , job ) </> When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job Returns (bool, optional) False to cancel submission function pipen.plugin . on_job_submitted ( proc , job ) </> When a job is submitted in the scheduler system. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function pipen.plugin . on_job_running ( proc , job ) </> When a job starts to run in scheduler system. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function pipen.plugin . on_job_killing ( proc , job ) </> When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job Returns (bool, optional) False to cancel killing function pipen.plugin . on_job_killed ( proc , job ) </> When a job is killed Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function pipen.plugin . on_job_succeeded ( proc , job ) </> When a job completes successfully. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function pipen.plugin . on_job_failed ( proc , job ) </> When a job is done but failed. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job class pipen.plugin . PipenMainPlugin ( ) </> The builtin main plugin, used to update the progress bar and cache the job class pipen.plugin . XqutePipenPlugin ( ) </> The plugin for xqute working as proxy for pipen plugin hooks","title":"pipen.plugin"},{"location":"api/pipen.plugin/#pipenplugin","text":"</> Define hooks specifications and provide plugin manager Classes PipenMainPlugin \u2014 The builtin main plugin, used to update the progress bar and cache the job </> XqutePipenPlugin \u2014 The plugin for xqute working as proxy for pipen plugin hooks </> Functions on_complete ( pipen ) \u2014 The the pipeline is complete. </> on_init ( pipen ) \u2014 The the pipeline is initialized. </> on_job_failed ( proc , job ) \u2014 When a job is done but failed. </> on_job_init ( proc , job ) \u2014 When a job is initialized </> on_job_killed ( proc , job ) \u2014 When a job is killed </> on_job_killing ( proc , job ) (bool, optional) \u2014 When a job is being killed. </> on_job_queued ( proc , job ) \u2014 When a job is queued in xqute. Note it might not be queued yet in the scheduler system. </> on_job_running ( proc , job ) \u2014 When a job starts to run in scheduler system. </> on_job_submitted ( proc , job ) \u2014 When a job is submitted in the scheduler system. </> on_job_submitting ( proc , job ) (bool, optional) \u2014 When a job is submitting. </> on_job_succeeded ( proc , job ) \u2014 When a job completes successfully. </> on_proc_done ( proc ) \u2014 When a process is done </> on_proc_init ( proc ) \u2014 When a process is initialized </> on_proc_shutdown ( proc , sig ) \u2014 When pipeline is shutting down, by Ctrl-c for example. </> on_setup ( plugin_config ) \u2014 Setup for plugins, primarily used for the plugins to setup some default configurations </> function","title":"pipen.plugin"},{"location":"api/pipen.plugin/#pipenpluginon_setup","text":"</> Setup for plugins, primarily used for the plugins to setup some default configurations Parameters plugin_config (dict(str: any)) \u2014 The plugin configuration dictionary One should define a configuration item either with a prefix as the identity for the plugin or a namespace inside the plugin config. function","title":"pipen.plugin.on_setup"},{"location":"api/pipen.plugin/#pipenpluginon_init","text":"</> The the pipeline is initialized. Parameters pipen (Pipen) \u2014 The Pipen object function","title":"pipen.plugin.on_init"},{"location":"api/pipen.plugin/#pipenpluginon_complete","text":"</> The the pipeline is complete. Note that this hook is only called when the pipeline is successfully completed Parameters pipen (Pipen) \u2014 The Pipen object function","title":"pipen.plugin.on_complete"},{"location":"api/pipen.plugin/#pipenpluginon_proc_init","text":"</> When a process is initialized Parameters proc (Proc) \u2014 The process function","title":"pipen.plugin.on_proc_init"},{"location":"api/pipen.plugin/#pipenpluginon_proc_shutdown","text":"</> When pipeline is shutting down, by Ctrl-c for example. Return False to stop shutting down, but you have to shut it down by yourself, for example, proc.xqute.task.cancel() Only the first return value will be used. Parameters sig (Signals, optional) \u2014 The signal. None means a natural shutdown pipen \u2014 The xqute object function","title":"pipen.plugin.on_proc_shutdown"},{"location":"api/pipen.plugin/#pipenpluginon_proc_done","text":"</> When a process is done This hook will be called anyway when a proc is succeeded or failed. To check if the process is succeeded, use proc.succeeded Parameters proc (Proc) \u2014 The process function","title":"pipen.plugin.on_proc_done"},{"location":"api/pipen.plugin/#pipenpluginon_job_init","text":"</> When a job is initialized Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function","title":"pipen.plugin.on_job_init"},{"location":"api/pipen.plugin/#pipenpluginon_job_queued","text":"</> When a job is queued in xqute. Note it might not be queued yet in the scheduler system. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function","title":"pipen.plugin.on_job_queued"},{"location":"api/pipen.plugin/#pipenpluginon_job_submitting","text":"</> When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job Returns (bool, optional) False to cancel submission function","title":"pipen.plugin.on_job_submitting"},{"location":"api/pipen.plugin/#pipenpluginon_job_submitted","text":"</> When a job is submitted in the scheduler system. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function","title":"pipen.plugin.on_job_submitted"},{"location":"api/pipen.plugin/#pipenpluginon_job_running","text":"</> When a job starts to run in scheduler system. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function","title":"pipen.plugin.on_job_running"},{"location":"api/pipen.plugin/#pipenpluginon_job_killing","text":"</> When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job Returns (bool, optional) False to cancel killing function","title":"pipen.plugin.on_job_killing"},{"location":"api/pipen.plugin/#pipenpluginon_job_killed","text":"</> When a job is killed Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function","title":"pipen.plugin.on_job_killed"},{"location":"api/pipen.plugin/#pipenpluginon_job_succeeded","text":"</> When a job completes successfully. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job function","title":"pipen.plugin.on_job_succeeded"},{"location":"api/pipen.plugin/#pipenpluginon_job_failed","text":"</> When a job is done but failed. Parameters proc (Proc) \u2014 The process job (Job) \u2014 The job class","title":"pipen.plugin.on_job_failed"},{"location":"api/pipen.plugin/#pipenpluginpipenmainplugin","text":"</> The builtin main plugin, used to update the progress bar and cache the job class","title":"pipen.plugin.PipenMainPlugin"},{"location":"api/pipen.plugin/#pipenpluginxqutepipenplugin","text":"</> The plugin for xqute working as proxy for pipen plugin hooks","title":"pipen.plugin.XqutePipenPlugin"},{"location":"api/pipen.proc/","text":"module pipen . proc </> Provide the Proc class Classes Proc \u2014 The Proc class provides process assembly functionality </> class pipen.proc . Proc ( *args , **kwargs ) </> Bases pipen._proc_properties.ProcProperties The Proc class provides process assembly functionality Classes ProcMeta \u2014 The metaclass for Proc class </> Methods __new__ ( cls , *args , **kwargs ) \u2014 Make sure cls() always get to the same instance </> compute_properties ( ) \u2014 Compute some properties </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> prepare ( pipeline , profile ) \u2014 Prepare the process </> properties_from_config ( config ) \u2014 Inherit properties from configuration if they are not set in the class variables or constructor </> run ( ) \u2014 Run the process </> class pipen._proc_properties . ProcMeta ( name , bases , dct ) </> The metaclass for Proc class We need it to invoke _compute_requires so that the class variable requires can chain the processes. method properties_from_config ( config ) </> Inherit properties from configuration if they are not set in the class variables or constructor Parameters config (Config) \u2014 The configuration method compute_properties ( ) </> Compute some properties staticmethod __new__ ( cls , *args , **kwargs ) </> Make sure cls() always get to the same instance method log ( level , msg , *args , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (Logger, optional) \u2014 The logging logger method gc ( ) </> GC process for the process to save memory after it's done method prepare ( pipeline , profile ) </> Prepare the process Parameters pipeline (Pipen) \u2014 The Pipen object profile (str) \u2014 The profile of the configuration method run ( ) </> Run the process","title":"pipen.proc"},{"location":"api/pipen.proc/#pipenproc","text":"</> Provide the Proc class Classes Proc \u2014 The Proc class provides process assembly functionality </> class","title":"pipen.proc"},{"location":"api/pipen.proc/#pipenprocproc","text":"</> Bases pipen._proc_properties.ProcProperties The Proc class provides process assembly functionality Classes ProcMeta \u2014 The metaclass for Proc class </> Methods __new__ ( cls , *args , **kwargs ) \u2014 Make sure cls() always get to the same instance </> compute_properties ( ) \u2014 Compute some properties </> gc ( ) \u2014 GC process for the process to save memory after it's done </> log ( level , msg , *args , logger ) \u2014 Log message for the process </> prepare ( pipeline , profile ) \u2014 Prepare the process </> properties_from_config ( config ) \u2014 Inherit properties from configuration if they are not set in the class variables or constructor </> run ( ) \u2014 Run the process </> class","title":"pipen.proc.Proc"},{"location":"api/pipen.proc/#pipen_proc_propertiesprocmeta","text":"</> The metaclass for Proc class We need it to invoke _compute_requires so that the class variable requires can chain the processes. method","title":"pipen._proc_properties.ProcMeta"},{"location":"api/pipen.proc/#pipen_proc_propertiesprocpropertiesproperties_from_config","text":"</> Inherit properties from configuration if they are not set in the class variables or constructor Parameters config (Config) \u2014 The configuration method","title":"pipen._proc_properties.ProcProperties.properties_from_config"},{"location":"api/pipen.proc/#pipen_proc_propertiesprocpropertiescompute_properties","text":"</> Compute some properties staticmethod","title":"pipen._proc_properties.ProcProperties.compute_properties"},{"location":"api/pipen.proc/#pipenprocprocnew","text":"</> Make sure cls() always get to the same instance method","title":"pipen.proc.Proc.new"},{"location":"api/pipen.proc/#pipenprocproclog","text":"</> Log message for the process Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message logger (Logger, optional) \u2014 The logging logger method","title":"pipen.proc.Proc.log"},{"location":"api/pipen.proc/#pipenprocprocgc","text":"</> GC process for the process to save memory after it's done method","title":"pipen.proc.Proc.gc"},{"location":"api/pipen.proc/#pipenprocprocprepare","text":"</> Prepare the process Parameters pipeline (Pipen) \u2014 The Pipen object profile (str) \u2014 The profile of the configuration method","title":"pipen.proc.Proc.prepare"},{"location":"api/pipen.proc/#pipenprocprocrun","text":"</> Run the process","title":"pipen.proc.Proc.run"},{"location":"api/pipen.progressbar/","text":"module pipen . progressbar </> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> class pipen.progressbar . ProcPBar ( manager , proc_size , proc_name ) </> The progress bar for processes Methods done ( ) \u2014 The process is done </> update_job_failed ( ) \u2014 Update the progress bar when a job is failed </> update_job_retrying ( ) \u2014 Update the progress bar when a job is retrying </> update_job_running ( ) \u2014 Update the progress bar when a job is running </> update_job_submitted ( ) \u2014 Update the progress bar when a job is submitted </> update_job_succeeded ( ) \u2014 Update the progress bar when a job is succeeded </> method update_job_submitted ( ) </> Update the progress bar when a job is submitted method update_job_retrying ( ) </> Update the progress bar when a job is retrying method update_job_running ( ) </> Update the progress bar when a job is running method update_job_succeeded ( ) </> Update the progress bar when a job is succeeded method update_job_failed ( ) </> Update the progress bar when a job is failed method done ( ) </> The process is done class pipen.progressbar . PipelinePBar ( n_procs , ppln_name , desc_len ) </> Progress bar for the pipeline Methods done ( ) \u2014 When the pipeline is done </> proc_bar ( proc_size , proc_name ) ( ProcPBar ) \u2014 Get the progress bar for a process </> update_proc_done ( ) \u2014 Update the progress bar when a process is done </> update_proc_error ( ) \u2014 Update the progress bar when a process is errored </> update_proc_running ( ) \u2014 Update the progress bar when a process is running </> method proc_bar ( proc_size , proc_name ) </> Get the progress bar for a process Parameters proc_size (int) \u2014 The size of the process proc_name (str) \u2014 The name of the process Returns ( ProcPBar ) The progress bar for the given process method update_proc_running ( ) </> Update the progress bar when a process is running method update_proc_done ( ) </> Update the progress bar when a process is done method update_proc_error ( ) </> Update the progress bar when a process is errored method done ( ) </> When the pipeline is done","title":"pipen.progressbar"},{"location":"api/pipen.progressbar/#pipenprogressbar","text":"</> Provide the PipelinePBar and ProcPBar classes Classes ProcPBar \u2014 The progress bar for processes </> PipelinePBar \u2014 Progress bar for the pipeline </> class","title":"pipen.progressbar"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbar","text":"</> The progress bar for processes Methods done ( ) \u2014 The process is done </> update_job_failed ( ) \u2014 Update the progress bar when a job is failed </> update_job_retrying ( ) \u2014 Update the progress bar when a job is retrying </> update_job_running ( ) \u2014 Update the progress bar when a job is running </> update_job_submitted ( ) \u2014 Update the progress bar when a job is submitted </> update_job_succeeded ( ) \u2014 Update the progress bar when a job is succeeded </> method","title":"pipen.progressbar.ProcPBar"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_submitted","text":"</> Update the progress bar when a job is submitted method","title":"pipen.progressbar.ProcPBar.update_job_submitted"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_retrying","text":"</> Update the progress bar when a job is retrying method","title":"pipen.progressbar.ProcPBar.update_job_retrying"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_running","text":"</> Update the progress bar when a job is running method","title":"pipen.progressbar.ProcPBar.update_job_running"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_succeeded","text":"</> Update the progress bar when a job is succeeded method","title":"pipen.progressbar.ProcPBar.update_job_succeeded"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbarupdate_job_failed","text":"</> Update the progress bar when a job is failed method","title":"pipen.progressbar.ProcPBar.update_job_failed"},{"location":"api/pipen.progressbar/#pipenprogressbarprocpbardone","text":"</> The process is done class","title":"pipen.progressbar.ProcPBar.done"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbar","text":"</> Progress bar for the pipeline Methods done ( ) \u2014 When the pipeline is done </> proc_bar ( proc_size , proc_name ) ( ProcPBar ) \u2014 Get the progress bar for a process </> update_proc_done ( ) \u2014 Update the progress bar when a process is done </> update_proc_error ( ) \u2014 Update the progress bar when a process is errored </> update_proc_running ( ) \u2014 Update the progress bar when a process is running </> method","title":"pipen.progressbar.PipelinePBar"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarproc_bar","text":"</> Get the progress bar for a process Parameters proc_size (int) \u2014 The size of the process proc_name (str) \u2014 The name of the process Returns ( ProcPBar ) The progress bar for the given process method","title":"pipen.progressbar.PipelinePBar.proc_bar"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarupdate_proc_running","text":"</> Update the progress bar when a process is running method","title":"pipen.progressbar.PipelinePBar.update_proc_running"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarupdate_proc_done","text":"</> Update the progress bar when a process is done method","title":"pipen.progressbar.PipelinePBar.update_proc_done"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbarupdate_proc_error","text":"</> Update the progress bar when a process is errored method","title":"pipen.progressbar.PipelinePBar.update_proc_error"},{"location":"api/pipen.progressbar/#pipenprogressbarpipelinepbardone","text":"</> When the pipeline is done","title":"pipen.progressbar.PipelinePBar.done"},{"location":"api/pipen.scheduler/","text":"module pipen . scheduler </> Provide builting schedulers Classes LocalJob \u2014 Job class for local scheduler </> LocalScheduler \u2014 Local scheduler </> SgeJob \u2014 Job class for sge scheduler </> SgeScheduler \u2014 Sge scheduler </> Functions get_scheduler ( scheduler ) (type of Scheduler) \u2014 Get the scheduler by name of the scheduler class itself </> class pipen.scheduler . LocalJob ( *args , **kwargs ) </> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for local scheduler Attributes CMD_WRAPPER_SHELL \u2014 The shell to run the wrapped script CMD_WRAPPER_TEMPLATE \u2014 The template for job wrapping _error_retry \u2014 Whether we should retry if error happened _num_retries \u2014 Total number of retries _rc \u2014 The return code of the job _status \u2014 The status of the job _wrapped_cmd \u2014 The wrapped cmd, used for job submission cached (bool) \u2014 check if a job is cached </> cmd \u2014 The command hook_done \u2014 Mark whether hooks have already been. Since we don't have a trigger for job finished/failed, so we do a polling on it. This is to avoid calling the hooks repeatedly index \u2014 The index of the job lock_file (Path) \u2014 The lock file of the job </> metadir \u2014 The metadir of the job rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> trial_count \u2014 The count for re-tries uid \u2014 The uid of the job in scheduler system uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method cache ( ) </> write signature to signature file class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method __repr__ ( ) \u2192 str </> repr of the job method clean ( retry=False ) </> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method wrapped_script ( scheduler ) </> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object class pipen.scheduler . LocalScheduler ( forks , prescript='' , postscript='' , **kwargs ) </> Bases xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler Local scheduler Parameters forks (int) \u2014 Max number of job forks prescript (str, optional) \u2014 The script to run before the command postscript (str, optional) \u2014 The script to run after the command **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class name \u2014 The name of the scheduler Classes LocalJob \u2014 Job class for local scheduler </> Methods job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.lock_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> kill_job ( job ) \u2014 Kill a job asynchronously </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> polling_jobs ( jobs , on , halt_on_error ) (bool) \u2014 Check if all jobs are done or new jobs can submit </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (int) \u2014 Submit a job locally </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> method submit_job_and_update_status ( job ) </> Submit and update the status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method polling_jobs ( jobs , on , halt_on_error ) </> Check if all jobs are done or new jobs can submit Parameters jobs (list of Job) \u2014 The list of jobs on (str) \u2014 query on status: can_submit or all_done halt_on_error (bool) \u2014 Whether we should halt the whole pipeline on error Returns (bool) True if yes otherwise False. method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (list of Job) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method submit_job ( job ) </> Submit a job locally Parameters job (Job) \u2014 The job Returns (int) The process id method kill_job ( job ) </> Kill a job asynchronously Parameters job (Job) \u2014 The job method job_is_running ( job ) </> Tell if a job is really running, not only the job.lock_file In case where the lockfile is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class pipen.scheduler . LocalJob ( *args , **kwargs ) </> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for local scheduler Attributes cached (bool) \u2014 check if a job is cached </> lock_file (Path) \u2014 The lock file of the job </> rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method cache ( ) </> write signature to signature file class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method __repr__ ( ) \u2192 str </> repr of the job method clean ( retry=False ) </> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method wrapped_script ( scheduler ) </> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object class pipen.scheduler . SgeJob ( *args , **kwargs ) </> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for sge scheduler Attributes CMD_WRAPPER_SHELL \u2014 The shell to run the wrapped script CMD_WRAPPER_TEMPLATE \u2014 The template for job wrapping _error_retry \u2014 Whether we should retry if error happened _num_retries \u2014 Total number of retries _rc \u2014 The return code of the job _status \u2014 The status of the job _wrapped_cmd \u2014 The wrapped cmd, used for job submission cached (bool) \u2014 check if a job is cached </> cmd \u2014 The command hook_done \u2014 Mark whether hooks have already been. Since we don't have a trigger for job finished/failed, so we do a polling on it. This is to avoid calling the hooks repeatedly index \u2014 The index of the job lock_file (Path) \u2014 The lock file of the job </> metadir \u2014 The metadir of the job rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> trial_count \u2014 The count for re-tries uid \u2014 The uid of the job in scheduler system uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method cache ( ) </> write signature to signature file class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method __repr__ ( ) \u2192 str </> repr of the job method clean ( retry=False ) </> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method wrapped_script ( scheduler ) </> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object class pipen.scheduler . SgeScheduler ( *args , **kwargs ) </> Bases xqute.schedulers.sge_scheduler.SgeScheduler xqute.scheduler.Scheduler Sge scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class name \u2014 The name of the scheduler Classes SgeJob \u2014 Job class for sge scheduler </> Methods job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.lock_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> kill_job ( job ) \u2014 Kill a job on SGE </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> polling_jobs ( jobs , on , halt_on_error ) (bool) \u2014 Check if all jobs are done or new jobs can submit </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to SGE </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> method submit_job_and_update_status ( job ) </> Submit and update the status Parameters job (Job) \u2014 The job method retry_job ( job ) </> Retry a job Parameters job (Job) \u2014 The job method kill_job_and_update_status ( job ) </> Kill a job and update its status Parameters job (Job) \u2014 The job method polling_jobs ( jobs , on , halt_on_error ) </> Check if all jobs are done or new jobs can submit Parameters jobs (list of Job) \u2014 The list of jobs on (str) \u2014 query on status: can_submit or all_done halt_on_error (bool) \u2014 Whether we should halt the whole pipeline on error Returns (bool) True if yes otherwise False. method kill_running_jobs ( jobs ) </> Try to kill all running jobs Parameters jobs (list of Job) \u2014 The list of jobs method job_is_submitted_or_running ( job ) </> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method submit_job ( job ) </> Submit a job to SGE Parameters job (Job) \u2014 The job Returns (str) The job id method kill_job ( job ) </> Kill a job on SGE Parameters job (Job) \u2014 The job method job_is_running ( job ) </> Tell if a job is really running, not only the job.lock_file In case where the lockfile is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class pipen.scheduler . SgeJob ( *args , **kwargs ) </> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for sge scheduler Attributes cached (bool) \u2014 check if a job is cached </> lock_file (Path) \u2014 The lock file of the job </> rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method cache ( ) </> write signature to signature file class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method __repr__ ( ) \u2192 str </> repr of the job method clean ( retry=False ) </> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method wrapped_script ( scheduler ) </> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object function pipen.scheduler . get_scheduler ( scheduler ) </> Get the scheduler by name of the scheduler class itself Parameters scheduler (Union(str, type of scheduler)) \u2014 The scheduler class or name Returns (type of Scheduler) The scheduler class","title":"pipen.scheduler"},{"location":"api/pipen.scheduler/#pipenscheduler","text":"</> Provide builting schedulers Classes LocalJob \u2014 Job class for local scheduler </> LocalScheduler \u2014 Local scheduler </> SgeJob \u2014 Job class for sge scheduler </> SgeScheduler \u2014 Sge scheduler </> Functions get_scheduler ( scheduler ) (type of Scheduler) \u2014 Get the scheduler by name of the scheduler class itself </> class","title":"pipen.scheduler"},{"location":"api/pipen.scheduler/#pipenschedulerlocaljob","text":"</> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for local scheduler Attributes CMD_WRAPPER_SHELL \u2014 The shell to run the wrapped script CMD_WRAPPER_TEMPLATE \u2014 The template for job wrapping _error_retry \u2014 Whether we should retry if error happened _num_retries \u2014 Total number of retries _rc \u2014 The return code of the job _status \u2014 The status of the job _wrapped_cmd \u2014 The wrapped cmd, used for job submission cached (bool) \u2014 check if a job is cached </> cmd \u2014 The command hook_done \u2014 Mark whether hooks have already been. Since we don't have a trigger for job finished/failed, so we do a polling on it. This is to avoid calling the hooks repeatedly index \u2014 The index of the job lock_file (Path) \u2014 The lock file of the job </> metadir \u2014 The metadir of the job rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> trial_count \u2014 The count for re-tries uid \u2014 The uid of the job in scheduler system uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method","title":"pipen.scheduler.LocalJob"},{"location":"api/pipen.scheduler/#pipen_job_cachingjobcachingcache","text":"</> write signature to signature file class","title":"pipen._job_caching.JobCaching.cache"},{"location":"api/pipen.scheduler/#abcabcmeta","text":"</> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method","title":"abc.ABCMeta"},{"location":"api/pipen.scheduler/#xqutejobjobrepr","text":"</> repr of the job method","title":"xqute.job.Job.repr"},{"location":"api/pipen.scheduler/#xqutejobjobclean","text":"</> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method","title":"xqute.job.Job.clean"},{"location":"api/pipen.scheduler/#xqutejobjobwrapped_script","text":"</> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method","title":"xqute.job.Job.wrapped_script"},{"location":"api/pipen.scheduler/#pipenjobjoblog","text":"</> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method","title":"pipen.job.Job.log"},{"location":"api/pipen.scheduler/#pipenjobjobprepare","text":"</> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object class","title":"pipen.job.Job.prepare"},{"location":"api/pipen.scheduler/#pipenschedulerlocalscheduler","text":"</> Bases xqute.schedulers.local_scheduler.LocalScheduler xqute.scheduler.Scheduler Local scheduler Parameters forks (int) \u2014 Max number of job forks prescript (str, optional) \u2014 The script to run before the command postscript (str, optional) \u2014 The script to run after the command **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class name \u2014 The name of the scheduler Classes LocalJob \u2014 Job class for local scheduler </> Methods job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.lock_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> kill_job ( job ) \u2014 Kill a job asynchronously </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> polling_jobs ( jobs , on , halt_on_error ) (bool) \u2014 Check if all jobs are done or new jobs can submit </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (int) \u2014 Submit a job locally </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> method","title":"pipen.scheduler.LocalScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status","text":"</> Submit and update the status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerpolling_jobs","text":"</> Check if all jobs are done or new jobs can submit Parameters jobs (list of Job) \u2014 The list of jobs on (str) \u2014 query on status: can_submit or all_done halt_on_error (bool) \u2014 Whether we should halt the whole pipeline on error Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.polling_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs","text":"</> Try to kill all running jobs Parameters jobs (list of Job) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulersubmit_job","text":"</> Submit a job locally Parameters job (Job) \u2014 The job Returns (int) The process id method","title":"xqute.schedulers.local_scheduler.LocalScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerkill_job","text":"</> Kill a job asynchronously Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.local_scheduler.LocalScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulerslocal_schedulerlocalschedulerjob_is_running","text":"</> Tell if a job is really running, not only the job.lock_file In case where the lockfile is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class","title":"xqute.schedulers.local_scheduler.LocalScheduler.job_is_running"},{"location":"api/pipen.scheduler/#pipenschedulerlocaljob_1","text":"</> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for local scheduler Attributes cached (bool) \u2014 check if a job is cached </> lock_file (Path) \u2014 The lock file of the job </> rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method cache ( ) </> write signature to signature file class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method __repr__ ( ) \u2192 str </> repr of the job method clean ( retry=False ) </> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method wrapped_script ( scheduler ) </> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object class","title":"pipen.scheduler.LocalJob"},{"location":"api/pipen.scheduler/#pipenschedulersgejob","text":"</> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for sge scheduler Attributes CMD_WRAPPER_SHELL \u2014 The shell to run the wrapped script CMD_WRAPPER_TEMPLATE \u2014 The template for job wrapping _error_retry \u2014 Whether we should retry if error happened _num_retries \u2014 Total number of retries _rc \u2014 The return code of the job _status \u2014 The status of the job _wrapped_cmd \u2014 The wrapped cmd, used for job submission cached (bool) \u2014 check if a job is cached </> cmd \u2014 The command hook_done \u2014 Mark whether hooks have already been. Since we don't have a trigger for job finished/failed, so we do a polling on it. This is to avoid calling the hooks repeatedly index \u2014 The index of the job lock_file (Path) \u2014 The lock file of the job </> metadir \u2014 The metadir of the job rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> trial_count \u2014 The count for re-tries uid \u2014 The uid of the job in scheduler system uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method","title":"pipen.scheduler.SgeJob"},{"location":"api/pipen.scheduler/#pipen_job_cachingjobcachingcache_1","text":"</> write signature to signature file class","title":"pipen._job_caching.JobCaching.cache"},{"location":"api/pipen.scheduler/#abcabcmeta_1","text":"</> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method","title":"abc.ABCMeta"},{"location":"api/pipen.scheduler/#xqutejobjobrepr_1","text":"</> repr of the job method","title":"xqute.job.Job.repr"},{"location":"api/pipen.scheduler/#xqutejobjobclean_1","text":"</> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method","title":"xqute.job.Job.clean"},{"location":"api/pipen.scheduler/#xqutejobjobwrapped_script_1","text":"</> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method","title":"xqute.job.Job.wrapped_script"},{"location":"api/pipen.scheduler/#pipenjobjoblog_1","text":"</> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method","title":"pipen.job.Job.log"},{"location":"api/pipen.scheduler/#pipenjobjobprepare_1","text":"</> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object class","title":"pipen.job.Job.prepare"},{"location":"api/pipen.scheduler/#pipenschedulersgescheduler","text":"</> Bases xqute.schedulers.sge_scheduler.SgeScheduler xqute.scheduler.Scheduler Sge scheduler Parameters **kwargs \u2014 Other arguments for the scheduler Attributes job_class \u2014 The job class name \u2014 The name of the scheduler Classes SgeJob \u2014 Job class for sge scheduler </> Methods job_is_running ( job ) (bool) \u2014 Tell if a job is really running, not only the job.lock_file </> job_is_submitted_or_running ( job ) (bool) \u2014 Check if a job is already submitted or running </> kill_job ( job ) \u2014 Kill a job on SGE </> kill_job_and_update_status ( job ) \u2014 Kill a job and update its status </> kill_running_jobs ( jobs ) \u2014 Try to kill all running jobs </> polling_jobs ( jobs , on , halt_on_error ) (bool) \u2014 Check if all jobs are done or new jobs can submit </> retry_job ( job ) \u2014 Retry a job </> submit_job ( job ) (str) \u2014 Submit a job to SGE </> submit_job_and_update_status ( job ) \u2014 Submit and update the status </> method","title":"pipen.scheduler.SgeScheduler"},{"location":"api/pipen.scheduler/#xquteschedulerschedulersubmit_job_and_update_status_1","text":"</> Submit and update the status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.submit_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerretry_job_1","text":"</> Retry a job Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.retry_job"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_job_and_update_status_1","text":"</> Kill a job and update its status Parameters job (Job) \u2014 The job method","title":"xqute.scheduler.Scheduler.kill_job_and_update_status"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerpolling_jobs_1","text":"</> Check if all jobs are done or new jobs can submit Parameters jobs (list of Job) \u2014 The list of jobs on (str) \u2014 query on status: can_submit or all_done halt_on_error (bool) \u2014 Whether we should halt the whole pipeline on error Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.polling_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerkill_running_jobs_1","text":"</> Try to kill all running jobs Parameters jobs (list of Job) \u2014 The list of jobs method","title":"xqute.scheduler.Scheduler.kill_running_jobs"},{"location":"api/pipen.scheduler/#xquteschedulerschedulerjob_is_submitted_or_running_1","text":"</> Check if a job is already submitted or running Parameters job (Job) \u2014 The job Returns (bool) True if yes otherwise False. method","title":"xqute.scheduler.Scheduler.job_is_submitted_or_running"},{"location":"api/pipen.scheduler/#xquteschedulerssge_schedulersgeschedulersubmit_job","text":"</> Submit a job to SGE Parameters job (Job) \u2014 The job Returns (str) The job id method","title":"xqute.schedulers.sge_scheduler.SgeScheduler.submit_job"},{"location":"api/pipen.scheduler/#xquteschedulerssge_schedulersgeschedulerkill_job","text":"</> Kill a job on SGE Parameters job (Job) \u2014 The job method","title":"xqute.schedulers.sge_scheduler.SgeScheduler.kill_job"},{"location":"api/pipen.scheduler/#xquteschedulerssge_schedulersgeschedulerjob_is_running","text":"</> Tell if a job is really running, not only the job.lock_file In case where the lockfile is not cleaned when job is done. Parameters job (Job) \u2014 The job Returns (bool) True if it is, otherwise False class","title":"xqute.schedulers.sge_scheduler.SgeScheduler.job_is_running"},{"location":"api/pipen.scheduler/#pipenschedulersgejob_1","text":"</> Bases pipen.job.Job xqute.job.Job abc.ABC pipen._job_caching.JobCaching Job class for sge scheduler Attributes cached (bool) \u2014 check if a job is cached </> lock_file (Path) \u2014 The lock file of the job </> rc (int) \u2014 The return code of the job </> rc_file (Path) \u2014 The rc file of the job </> retry_dir (Path) \u2014 The retry directory of the job </> script_file (Path) \u2014 Get the path to script file </> signature_file (Path) \u2014 Get the path to the signature file </> status (int) \u2014 Query the status of the job If the job is submitted, try to query it from the status file Make sure the status is updated by trap in wrapped script </> status_file (Path) \u2014 The status file of the job </> stderr_file (Path) \u2014 The stderr file of the job </> stdout_file (Path) \u2014 The stdout file of the job </> strcmd (str) \u2014 Get the string representation of the command </> uid (str) \u2014 Get the uid of the job in scheduler system </> Classes ABCMeta \u2014 Metaclass for defining Abstract Base Classes (ABCs). </> Methods __repr__ ( ) (str) \u2014 repr of the job </> cache ( ) \u2014 write signature to signature file </> clean ( retry ) \u2014 Clean up the meta files </> log ( level , msg , *args , limit , limit_indicator , logger ) \u2014 Log message for the jobs </> prepare ( proc ) \u2014 Prepare the job by given process </> wrapped_script ( scheduler ) (PathLike) \u2014 Get the wrapped script </> method cache ( ) </> write signature to signature file class abc. ABCMeta ( name , bases , namespace , **kwargs ) </> Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Methods __instancecheck__ ( cls , instance ) \u2014 Override for isinstance(instance, cls). </> __subclasscheck__ ( cls , subclass ) \u2014 Override for issubclass(subclass, cls). </> register ( cls , subclass ) \u2014 Register a virtual subclass of an ABC. </> staticmethod register ( cls , subclass ) </> Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. staticmethod __instancecheck__ ( cls , instance ) </> Override for isinstance(instance, cls). staticmethod __subclasscheck__ ( cls , subclass ) </> Override for issubclass(subclass, cls). method __repr__ ( ) \u2192 str </> repr of the job method clean ( retry=False ) </> Clean up the meta files Parameters retry (optional) \u2014 Whether clean it for retrying method wrapped_script ( scheduler ) </> Get the wrapped script Parameters scheduler (Scheduler) \u2014 The scheduler Returns (PathLike) The path of the wrapped script method log ( level , msg , *args , limit=3 , limit_indicator=True , logger=<LoggerAdapter pipen.main (WARNING)> ) </> Log message for the jobs Parameters level (int or str) \u2014 The log level of the record msg (str) \u2014 The message to log *args \u2014 The arguments to format the message limit (int, optional) \u2014 limitation of the log (don't log for all jobs) limit_indicator (bool, optional) \u2014 Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger (Logger, optional) \u2014 The logger used to log method prepare ( proc ) </> Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Parameters proc (Proc) \u2014 the process object function","title":"pipen.scheduler.SgeJob"},{"location":"api/pipen.scheduler/#pipenschedulerget_scheduler","text":"</> Get the scheduler by name of the scheduler class itself Parameters scheduler (Union(str, type of scheduler)) \u2014 The scheduler class or name Returns (type of Scheduler) The scheduler class","title":"pipen.scheduler.get_scheduler"},{"location":"api/pipen.template/","text":"module pipen . template </> Template adaptor for pipen Classes Template ( source , **envs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (type of Template ) \u2014 Get the template engine by name or the template engine itself </> abstract class pipen.template . Template ( source , **envs ) </> Base class wrapper to wrap template for pipen Methods render ( data ) (str) \u2014 @API Render the template @parmas: data (dict): The data used to render </> update_envs ( **envs ) \u2014 @API Register extra environment @params: **envs: The environment </> method update_envs ( **envs ) </> @API Register extra environment @params: **envs: The environment method render ( data=None ) \u2192 str </> @API Render the template @parmas: data (dict): The data used to render class pipen.template . TemplateLiquid ( source , **envs ) </> Bases pipen.template.Template Liquidpy template wrapper. Methods render ( data ) (str) \u2014 @API Render the template @parmas: data (dict): The data used to render </> update_envs ( **envs ) \u2014 @API Register extra environment @params: **envs: The environment </> method update_envs ( **envs ) </> @API Register extra environment @params: **envs: The environment method render ( data=None ) \u2192 str </> @API Render the template @parmas: data (dict): The data used to render class pipen.template . TemplateJinja2 ( source , **envs ) </> Bases pipen.template.Template Jinja2 template wrapper Methods render ( data ) (str) \u2014 @API Render the template @parmas: data (dict): The data used to render </> update_envs ( **envs ) \u2014 @API Register extra environment @params: **envs: The environment </> method update_envs ( **envs ) </> @API Register extra environment @params: **envs: The environment method render ( data=None ) \u2192 str </> @API Render the template @parmas: data (dict): The data used to render function pipen.template . get_template_engine ( template ) </> Get the template engine by name or the template engine itself Parameters template (Union(str, type of template )) \u2014 The name of the template engine or the template engine itself Returns (type of Template ) The template engine","title":"pipen.template"},{"location":"api/pipen.template/#pipentemplate","text":"</> Template adaptor for pipen Classes Template ( source , **envs ) \u2014 Base class wrapper to wrap template for pipen </> TemplateLiquid \u2014 Liquidpy template wrapper. </> TemplateJinja2 \u2014 Jinja2 template wrapper </> Functions get_template_engine ( template ) (type of Template ) \u2014 Get the template engine by name or the template engine itself </> abstract class","title":"pipen.template"},{"location":"api/pipen.template/#pipentemplatetemplate","text":"</> Base class wrapper to wrap template for pipen Methods render ( data ) (str) \u2014 @API Render the template @parmas: data (dict): The data used to render </> update_envs ( **envs ) \u2014 @API Register extra environment @params: **envs: The environment </> method","title":"pipen.template.Template"},{"location":"api/pipen.template/#pipentemplatetemplateupdate_envs","text":"</> @API Register extra environment @params: **envs: The environment method","title":"pipen.template.Template.update_envs"},{"location":"api/pipen.template/#pipentemplatetemplaterender","text":"</> @API Render the template @parmas: data (dict): The data used to render class","title":"pipen.template.Template.render"},{"location":"api/pipen.template/#pipentemplatetemplateliquid","text":"</> Bases pipen.template.Template Liquidpy template wrapper. Methods render ( data ) (str) \u2014 @API Render the template @parmas: data (dict): The data used to render </> update_envs ( **envs ) \u2014 @API Register extra environment @params: **envs: The environment </> method","title":"pipen.template.TemplateLiquid"},{"location":"api/pipen.template/#pipentemplatetemplateupdate_envs_1","text":"</> @API Register extra environment @params: **envs: The environment method","title":"pipen.template.Template.update_envs"},{"location":"api/pipen.template/#pipentemplatetemplaterender_1","text":"</> @API Render the template @parmas: data (dict): The data used to render class","title":"pipen.template.Template.render"},{"location":"api/pipen.template/#pipentemplatetemplatejinja2","text":"</> Bases pipen.template.Template Jinja2 template wrapper Methods render ( data ) (str) \u2014 @API Render the template @parmas: data (dict): The data used to render </> update_envs ( **envs ) \u2014 @API Register extra environment @params: **envs: The environment </> method","title":"pipen.template.TemplateJinja2"},{"location":"api/pipen.template/#pipentemplatetemplateupdate_envs_2","text":"</> @API Register extra environment @params: **envs: The environment method","title":"pipen.template.Template.update_envs"},{"location":"api/pipen.template/#pipentemplatetemplaterender_2","text":"</> @API Render the template @parmas: data (dict): The data used to render function","title":"pipen.template.Template.render"},{"location":"api/pipen.template/#pipentemplateget_template_engine","text":"</> Get the template engine by name or the template engine itself Parameters template (Union(str, type of template )) \u2014 The name of the template engine or the template engine itself Returns (type of Template ) The template engine","title":"pipen.template.get_template_engine"},{"location":"api/pipen.utils/","text":"module pipen . utils </> Provide some utilities Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> get_console_width ( default , shift ) (int) \u2014 Get the console width </> get_logger ( name , level ) (Logger) \u2014 Get the logger by given plugin name </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path. </> get_plugin_context ( plugins ) (SimplugContext) \u2014 Get the plugin context to enable and disable plugins per pipeline </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of cls </> load_entrypoints ( group ) (iterable of (str, any)) \u2014 Load objects from setuptools entrypoints by given group name </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> pipen_banner ( ) (ConsoleRenderable, RichCast, or str) \u2014 The banner for pipen </> render_scope ( scope , title ) (ConsoleRenderable, RichCast, or str) \u2014 Log a mapping to console </> function pipen.utils . get_logger ( name , level=None ) </> Get the logger by given plugin name Parameters level (str, int, or NoneType, optional) \u2014 The initial level of the logger Returns (Logger) The logger function pipen.utils . get_console_width ( default=80 , shift=26 ) \u2192 int </> Get the console width Parameters default (int, optional) \u2014 The default console width if failed to get shift (int, optional) \u2014 The shift to subtract from the width as we have time, level, plugin name in log function pipen.utils . get_plugin_context ( plugins ) </> Get the plugin context to enable and disable plugins per pipeline Parameters plugins (list of any, optional) \u2014 A list of plugins to enable or a list of names with 'no:' as prefix to disable Returns (SimplugContext) The plugin context manager function pipen.utils . log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) </> Log a rich renderable to logger Parameters renderable (ConsoleRenderable, RichCast, or str) \u2014 The rich renderable logfunc (callable) \u2014 The log function, if message is not the first argument, use functools.partial to wrap it *args \u2014 The arguments to the log function **kwargs \u2014 The keyword arguments to the log function splitline \u2014 Whether split the lines or log the entire message function pipen.utils . render_scope ( scope , title ) \u2192 ConsoleRenderable, RichCast, or str </> Log a mapping to console Parameters scope \u2014 The mapping object title (str) \u2014 The title of the scope function pipen.utils . pipen_banner ( ) \u2192 ConsoleRenderable, RichCast, or str </> The banner for pipen function pipen.utils . brief_list ( blist ) </> Briefly show an integer list, combine the continuous numbers. Parameters blist (list of int) \u2014 The list Returns (str) The string to show for the briefed list. function pipen.utils . get_mtime ( path , dir_depth=1 ) </> Get the modification time of a path. If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth Parameters dir_depth (int, optional) \u2014 The depth of the directory to check the last modification time Returns (float) The last modification time of path function pipen.utils . is_subclass ( obj , cls ) </> Tell if obj is a subclass of cls Differences with issubclass is that we don't raise Type error if obj is not a class Parameters obj (any) \u2014 The object to check cls (type) \u2014 The class to check Returns (bool) True if obj is a subclass of cls otherwise False generator pipen.utils . load_entrypoints ( group ) </> Load objects from setuptools entrypoints by given group name Parameters group (str) \u2014 The group name of the entrypoints Returns (iterable of (str, any)) An iterable of tuples with name and the loaded object","title":"pipen.utils"},{"location":"api/pipen.utils/#pipenutils","text":"</> Provide some utilities Functions brief_list ( blist ) (str) \u2014 Briefly show an integer list, combine the continuous numbers. </> get_console_width ( default , shift ) (int) \u2014 Get the console width </> get_logger ( name , level ) (Logger) \u2014 Get the logger by given plugin name </> get_mtime ( path , dir_depth ) (float) \u2014 Get the modification time of a path. </> get_plugin_context ( plugins ) (SimplugContext) \u2014 Get the plugin context to enable and disable plugins per pipeline </> is_subclass ( obj , cls ) (bool) \u2014 Tell if obj is a subclass of cls </> load_entrypoints ( group ) (iterable of (str, any)) \u2014 Load objects from setuptools entrypoints by given group name </> log_rich_renderable ( renderable , color , logfunc , *args , **kwargs ) \u2014 Log a rich renderable to logger </> pipen_banner ( ) (ConsoleRenderable, RichCast, or str) \u2014 The banner for pipen </> render_scope ( scope , title ) (ConsoleRenderable, RichCast, or str) \u2014 Log a mapping to console </> function","title":"pipen.utils"},{"location":"api/pipen.utils/#pipenutilsget_logger","text":"</> Get the logger by given plugin name Parameters level (str, int, or NoneType, optional) \u2014 The initial level of the logger Returns (Logger) The logger function","title":"pipen.utils.get_logger"},{"location":"api/pipen.utils/#pipenutilsget_console_width","text":"</> Get the console width Parameters default (int, optional) \u2014 The default console width if failed to get shift (int, optional) \u2014 The shift to subtract from the width as we have time, level, plugin name in log function","title":"pipen.utils.get_console_width"},{"location":"api/pipen.utils/#pipenutilsget_plugin_context","text":"</> Get the plugin context to enable and disable plugins per pipeline Parameters plugins (list of any, optional) \u2014 A list of plugins to enable or a list of names with 'no:' as prefix to disable Returns (SimplugContext) The plugin context manager function","title":"pipen.utils.get_plugin_context"},{"location":"api/pipen.utils/#pipenutilslog_rich_renderable","text":"</> Log a rich renderable to logger Parameters renderable (ConsoleRenderable, RichCast, or str) \u2014 The rich renderable logfunc (callable) \u2014 The log function, if message is not the first argument, use functools.partial to wrap it *args \u2014 The arguments to the log function **kwargs \u2014 The keyword arguments to the log function splitline \u2014 Whether split the lines or log the entire message function","title":"pipen.utils.log_rich_renderable"},{"location":"api/pipen.utils/#pipenutilsrender_scope","text":"</> Log a mapping to console Parameters scope \u2014 The mapping object title (str) \u2014 The title of the scope function","title":"pipen.utils.render_scope"},{"location":"api/pipen.utils/#pipenutilspipen_banner","text":"</> The banner for pipen function","title":"pipen.utils.pipen_banner"},{"location":"api/pipen.utils/#pipenutilsbrief_list","text":"</> Briefly show an integer list, combine the continuous numbers. Parameters blist (list of int) \u2014 The list Returns (str) The string to show for the briefed list. function","title":"pipen.utils.brief_list"},{"location":"api/pipen.utils/#pipenutilsget_mtime","text":"</> Get the modification time of a path. If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth Parameters dir_depth (int, optional) \u2014 The depth of the directory to check the last modification time Returns (float) The last modification time of path function","title":"pipen.utils.get_mtime"},{"location":"api/pipen.utils/#pipenutilsis_subclass","text":"</> Tell if obj is a subclass of cls Differences with issubclass is that we don't raise Type error if obj is not a class Parameters obj (any) \u2014 The object to check cls (type) \u2014 The class to check Returns (bool) True if obj is a subclass of cls otherwise False generator","title":"pipen.utils.is_subclass"},{"location":"api/pipen.utils/#pipenutilsload_entrypoints","text":"</> Load objects from setuptools entrypoints by given group name Parameters group (str) \u2014 The group name of the entrypoints Returns (iterable of (str, any)) An iterable of tuples with name and the loaded object","title":"pipen.utils.load_entrypoints"},{"location":"api/source/pipen.channel/","text":"SOURCE CODE pipen. channel DOCS \"\"\"Provide some function for creating and modifying channels(dataframes)\"\"\" from typing import Any , List , Union from os import path from glob import glob import pandas from pandas import DataFrame from siuba.siu import _ __all__ = ( '_' , 'create' , 'from_glob' , 'from_pairs' , 'from_csv' , 'from_excel' , 'from_table' ) def create ( value : Union [ DataFrame , List [ Any ]]) -> DataFrame : DOCS \"\"\"Create a channel from a list. The second dimension is identified by tuple. if all elements are tuple, then a channel is created directly. Otherwise, elements are converted to tuples first and channels are created then. Examples: >>> Channel.create([1, 2, 3]) # 3 rows, 1 column >>> Channel.create([(1,2,3)]) # 1 row, 3 columns Args: value: The value to create a channel Returns: A channel (dataframe) \"\"\" if isinstance ( value , DataFrame ): return value if all ( isinstance ( elem , tuple ) for elem in value ): return DataFrame ( value ) return DataFrame (( val , ) for val in value ) def from_glob ( pattern , DOCS ftype = 'any' , sortby = 'name' , reverse = False ) -> DataFrame : \"\"\"Create a channel with a glob pattern Args: ftype: The file type, one of any, link, dir and file sortby: How the files should be sorted. One of name, mtime and size reverse: Whether sort them in a reversed way. Returns: The channel \"\"\" sort_key = ( str if sortby == 'name' else path . getmtime if sortby == 'mtime' else path . getsize if sortby == 'size' else None ) file_filter = ( path . islink if ftype == 'link' else path . isdir if ftype == 'dir' else path . isfile if ftype == 'file' else None ) files = ( file for file in glob ( str ( pattern )) if not file_filter or file_filter ( file )) return create ( sorted ( files , key = sort_key , reverse = reverse )) def from_pairs ( pattern , DOCS ftype = 'any' , sortby = 'name' , reverse = False ) -> DataFrame : \"\"\"Create a width=2 channel with a glob pattern Args: ftype: The file type, one of any, link, dir and file sortby: How the files should be sorted. One of name, mtime and size reverse: Whether sort them in a reversed way. Returns: The channel \"\"\" mates = from_glob ( pattern , ftype , sortby , reverse ) return pandas . concat ( ( mates . iloc [:: 2 ] . reset_index ( drop = True ), mates . iloc [ 1 :: 2 ] . reset_index ( drop = True )), axis = 1 ) # pylint: disable=invalid-name from_csv = pandas . read_csv from_csv . __doc__ = ( \"Create a channel from a csv file. \\n\\n \" f \" { pandas . read_csv . __doc__ } \" ) from_excel = pandas . read_excel from_excel . __doc__ = ( \"Create a channel from an excel file. \\n\\n \" f \" { pandas . read_excel . __doc__ } \" ) from_table = pandas . read_table from_table . __doc__ = ( \"Create a channel from a table file. \\n\\n \" f \" { pandas . read_table . __doc__ } \" )","title":"pipen.channel"},{"location":"api/source/pipen.channel.vector/","text":"SOURCE CODE pipen.channel. vector DOCS \"\"\"Vector help functions for pipen channels\"\"\" # pylint: disable=wildcard-import,unused-wildcard-import from siuba.dply.vector import *","title":"pipen.channel.vector"},{"location":"api/source/pipen.channel.verbs/","text":"SOURCE CODE pipen.channel. verbs DOCS \"\"\"Verbs for pipen channels\"\"\" from os import path from typing import Union import pandas from pandas import DataFrame import siuba from siuba.dply.verbs import singledispatch2 # pylint: disable=redefined-builtin,wildcard-import,unused-wildcard-import from siuba.dply.verbs import * from . import from_glob __all__ = tuple ( siuba . dply . verbs . __all__ ) + ( 'expand_dir' , 'collapse_files' ) # some useful pipe verbs @singledispatch2 ( DataFrame ) DOCS def expand_dir ( data : DataFrame , col : Union [ str , int ] = 0 , pattern : str = '*' , ftype : str = 'any' , sortby : str = 'name' , reverse : bool = False ) -> DataFrame : \"\"\"Expand a Channel according to the files in <col>, other cols will keep the same. This is only applicable to a 1-row channel. Examples: >>> ch = channel.create([('./', 1)]) >>> ch >> expand() >>> [['./a', 1], ['./b', 1], ['./c', 1]] Args: col: the index or name of the column used to expand pattern: use a pattern to filter the files/dirs, default: `*` ftype: the type of the files/dirs to include - 'dir', 'file', 'link' or 'any' (default) sortby: how the list is sorted - 'name' (default), 'mtime', 'size' reverse: reverse sort. Returns: The expanded channel \"\"\" assert data . shape [ 0 ] == 1 , \"Can only expand a single row DataFrame.\" col_loc = col if isinstance ( col , int ) else data . columns . get_loc ( col ) full_pattern = f \" { data . iloc [ 0 , col_loc ] } / { pattern } \" expanded = from_glob ( full_pattern , ftype , sortby , reverse ) ret = pandas . concat ([ data ] * expanded . size , axis = 0 ) ret [ data . columns [ col_loc ]] = expanded . values ret . reset_index ( drop = True ) return ret @singledispatch2 ( DataFrame ) DOCS def collapse_files ( data : DataFrame , col : Union [ str , int ] = 0 ) -> DataFrame : \"\"\"Collapse a Channel according to the files in <col>, other cols will use the values in row 0. Note that other values in other rows will be discarded. Examples: >>> ch = channel.create([['./a', 1], ['./b', 1], ['./c', 1]]) >>> ch >> collapse() >>> [['.', 1]] Args: data: The original channel col: the index or name of the column used to collapse on Returns: The collapsed channel \"\"\" assert data . shape [ 0 ] > 0 , \"Cannot collapse on an empty DataFrame.\" col_loc = col if isinstance ( col , int ) else data . columns . get_loc ( col ) paths = list ( data . iloc [:, col_loc ]) compx = path . dirname ( path . commonprefix ( paths )) ret = data . iloc [[ 0 ], :] . copy () ret . iloc [ 0 , col_loc ] = compx return ret","title":"pipen.channel.verbs"},{"location":"api/source/pipen.cli/","text":"SOURCE CODE pipen. cli DOCS \"\"\"Provide cli for pipen\"\"\" # pragma: no cover def main (): DOCS \"\"\"Main entry point for cli\"\"\" print ( 'TODO' )","title":"pipen.cli"},{"location":"api/source/pipen.defaults/","text":"SOURCE CODE pipen. defaults DOCS \"\"\"Provide some default values/objects\"\"\" from pathlib import Path from typing import ClassVar from diot import Diot from simpleconf import Config import uvloop from xqute import logger as xqute_logger , JobErrorStrategy # turn xqute's logger off xqute_logger . setLevel ( 100 ) xqute_logger . removeHandler ( xqute_logger . handlers [ 0 ]) uvloop . install () LOGGER_NAME = 'main' DEFAULT_CONFIG_FILES = ( Path ( '~/.pipen.toml' ), './.pipen.toml' , 'PIPEN.osenv' ) DEFAULT_CONFIG = Diot ( loglevel = 'debug' , # The cache option, True/False/export cache = True , # Whether expand directory to check signature dirsig = 1 , # How to deal with the errors # retry, ignore, halt # halt to halt the whole pipeline, no submitting new jobs # terminate to just terminate the job itself error_strategy = JobErrorStrategy . IGNORE , # How many times to retry to jobs once error occurs num_retries = 3 , # The directory to export the output files forks = 1 , # Default shell/language lang = 'bash' , # How many jobs to be submitted in a batch submission_batch = 8 , # The working directory for the pipeline workdir = './.pipen' , # template engine template = 'liquid' , # template envs envs = {}, # scheduler scheduler = 'local' , # scheduler options scheduler_opts = {}, # plugins plugins = None , # plugin opts plugin_opts = {} ) DEFAULT_CONSOLE_WIDTH : int = 80 DEFAULT_CONSOLE_WIDTH_SHIFT : int = 26 SCHEDULER_ENTRY_GROUP = 'pipen-sched' TEMPLATE_ENTRY_GROUP = 'pipen-tpl' class ProcInputType : DOCS \"\"\"Types for process inputs\"\"\" VAR : ClassVar [ str ] = 'var' FILE : ClassVar [ str ] = 'file' FILES : ClassVar [ str ] = 'files' class ProcOutputType : DOCS \"\"\"Types for process outputs\"\"\" VAR : ClassVar [ str ] = 'var' FILE : ClassVar [ str ] = 'file' config = Config () # pylint: disable=invalid-name","title":"pipen.defaults"},{"location":"api/source/pipen.exceptions/","text":"SOURCE CODE pipen. exceptions DOCS \"\"\"Provide exception classes\"\"\" class PipenException ( Exception ): DOCS \"\"\"Base exception class for pipen\"\"\" class ProcInputTypeError ( PipenException , TypeError ): DOCS \"\"\"When an unsupported input type is provided\"\"\" class ProcScriptFileNotFound ( PipenException , FileNotFoundError ): DOCS \"\"\"When script file specified as 'file://' cannot be found\"\"\" class ProcOutputNameError ( PipenException , NameError ): DOCS \"\"\"When no name or malformatted output is provided\"\"\" class ProcOutputTypeError ( PipenException , TypeError ): DOCS \"\"\"When an unsupported output type is provided\"\"\" class ProcOutputValueError ( PipenException , ValueError ): DOCS \"\"\"When a malformatted output value is provided\"\"\" class ProcDependencyError ( PipenException ): DOCS \"\"\"When there is something wrong the process dependencies\"\"\" class NoSuchSchedulerError ( PipenException ): DOCS \"\"\"When specified scheduler cannot be found\"\"\" class WrongSchedulerTypeError ( PipenException , TypeError ): DOCS \"\"\"When specified scheduler is not a subclass of Scheduler\"\"\" class NoSuchTemplateEngineError ( PipenException ): DOCS \"\"\"When specified template engine cannot be found\"\"\" class WrongTemplateEnginTypeError ( PipenException , TypeError ): DOCS \"\"\"When specified tempalte engine is not a subclass of Scheduler\"\"\" class ConfigurationError ( PipenException ): DOCS \"\"\"When something wrong set as configuration\"\"\" class ProcWorkdirConflictException ( PipenException ): DOCS \"\"\"\"When more than one processes are sharing the same workdir\"\"\"","title":"pipen.exceptions"},{"location":"api/source/pipen.job/","text":"SOURCE CODE pipen. job DOCS \"\"\"Provide the Job class\"\"\" import logging from os import PathLike from pathlib import Path from typing import Any , Dict , Union from diot import OrderedDiot from xqute import Job as XquteJob from xqute.utils import a_read_text from .defaults import ProcInputType , ProcOutputType from .utils import logger , cached_property # pylint: disable=unused-import from .exceptions import ( ProcInputTypeError , ProcOutputNameError , ProcOutputTypeError , ProcOutputValueError ) from .template import Template from ._job_caching import JobCaching class Job ( XquteJob , JobCaching ): DOCS \"\"\"The job for pipen\"\"\" # pylint: disable=redefined-outer-name __slots__ = ( 'proc' , '_output_types' , '_outdir' ) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . proc = None self . _output_types = {} self . _outdir = self . metadir / 'output' @property DOCS def script_file ( self ) -> Path : \"\"\"Get the path to script file\"\"\" return self . metadir / 'job.script' @cached_property def outdir ( self ) -> Path : \"\"\"Get the path to the output directory\"\"\" ret = Path ( self . _outdir ) ret . mkdir ( parents = True , exist_ok = True ) return ret @cached_property def input ( self ) -> Dict [ str , Any ]: \"\"\"Get the input data\"\"\" ret = self . proc . input . data . iloc [[ self . index ], :] . to_dict ( 'records' )[ 0 ] # check types for inkey , intype in self . proc . input . type . items (): if intype == ProcInputType . VAR : continue if intype == ProcInputType . FILE : if not isinstance ( ret [ inkey ], ( str , PathLike )): raise ProcInputTypeError ( f \"Got { type ( ret [ inkey ]) } instead of PathLike object \" f \"for input: { inkey + ':' + intype !r} \" ) if not Path ( ret [ inkey ]) . exists (): raise FileNotFoundError ( f \"Input file not found: { ret [ inkey ] } \" ) # we should use it as a string ret [ inkey ] = str ( ret [ inkey ]) if intype == ProcInputType . FILES : if not isinstance ( ret [ inkey ], ( list , tuple )): raise ProcInputTypeError ( \"Expected a list/tuple for input: \" f \" { inkey + ':' + intype !r} \" ) for i , file in enumerate ( ret [ inkey ]): if not Path ( file ) . exists (): raise FileNotFoundError ( f \"Input file not found: { file } \" ) ret [ inkey ][ i ] = str ( file ) return ret @cached_property def output ( self ) -> Dict [ str , Any ]: \"\"\"Get the output data\"\"\" output_template = self . proc . output if not output_template : return {} data = { 'job' : dict ( index = self . index , metadir = self . metadir , outdir = self . outdir , stdout_file = self . stdout_file , stderr_file = self . stderr_file , lock_file = self . lock_file , ), 'in' : self . input , 'proc' : self . proc , 'args' : self . proc . args } if isinstance ( output_template , Template ): # // TODO: check ',' in output value? outputs = [ oput . strip () for oput in output_template . render ( data ) . split ( ',' )] else : outputs = [ oput . render ( data ) for oput in output_template ] ret = OrderedDiot () for oput in outputs : if ':' not in oput : raise ProcOutputNameError ( 'No name given in output.' ) if oput . count ( ':' ) == 1 : output_name , output_value = oput . split ( ':' ) output_type = ProcOutputType . VAR else : output_name , output_type , output_value = oput . split ( ':' , 2 ) if output_type not in ProcOutputType . __dict__ . values (): raise ProcOutputTypeError ( f 'Unsupported output type: { output_type } ' ) self . _output_types [ output_name ] = output_type ret [ output_name ] = output_value if output_type == ProcOutputType . VAR : continue if '/' in output_value and self . proc . end : raise ProcOutputValueError ( 'Only basename allowed as output for ending process. ' 'If you want to redirect the output path, set `end` to ' 'False for the process.' ) if '/' in output_value : ret [ output_name ] = output_value else : ret [ output_name ] = str ( self . outdir . resolve () / output_value ) return ret @cached_property def rendering_data ( self ) -> Dict [ str , Any ]: \"\"\"Get the data for template rendering\"\"\" return { 'job' : dict ( index = self . index , metadir = self . metadir , outdir = self . outdir , stdout_file = self . stdout_file , stderr_file = self . stderr_file , lock_file = self . lock_file , ), 'in' : self . input , 'out' : self . output , 'proc' : self . proc , 'args' : self . proc . args } def log ( self , DOCS level : Union [ int , str ], msg : str , * args , limit : int = 3 , limit_indicator : bool = True , logger : logging . Logger = logger ) -> None : \"\"\"Log message for the jobs Args: level: The log level of the record msg: The message to log *args: The arguments to format the message limit: limitation of the log (don't log for all jobs) limit_indicator: Whether to show an indicator saying the log has been limited (the level of the indicator will be DEBUG) logger: The logger used to log \"\"\" if self . index > limit : return if self . index == limit : if limit_indicator : self . proc . log ( 'debug' , 'Not showing similar logs for further jobs.' ) return job_index_indicator = '[ %s / %s ] ' % ( str ( self . index ) . zfill ( len ( str ( self . proc . size - 1 ))), self . proc . size - 1 ) self . proc . log ( level , job_index_indicator + msg , * args , logger = logger ) async def prepare ( self , proc : \"Proc\" ) -> None : DOCS \"\"\"Prepare the job by given process Primarily prepare the script, and provide cmd to the job for xqute to wrap and run Args: proc: the process object \"\"\" self . proc = proc if self . proc . end and len ( self . proc . jobs ) == 1 : self . _outdir = Path ( self . proc . pipeline . outdir ) / self . proc . name elif self . proc . end : self . _outdir = ( Path ( self . proc . pipeline . outdir ) / self . proc . name / str ( self . index )) if not proc . script : self . cmd = [] # pylint: disable=attribute-defined-outside-init return script = proc . script . render ( self . rendering_data ) if self . script_file . is_file () and await a_read_text ( self . script_file ) != script : self . log ( 'debug' , 'Job script updated.' ) self . script_file . write_text ( script ) elif not self . script_file . is_file (): self . script_file . write_text ( script ) # pylint: disable=attribute-defined-outside-init self . cmd = [ proc . lang , self . script_file ]","title":"pipen.job"},{"location":"api/source/pipen/","text":"SOURCE CODE pipen DOCS \"\"\"Entrance of the package.\"\"\" from .defaults import config from .pipen import Pipen from .proc import Proc __version__ = '0.0.1'","title":"pipen"},{"location":"api/source/pipen.pipen/","text":"SOURCE CODE pipen. pipen DOCS \"\"\"Main entry module, provide the Pipen class\"\"\" from os import PathLike from typing import ClassVar , List , Optional , Union import asyncio from rich import box from rich.panel import Panel from slugify import slugify from .defaults import DEFAULT_CONFIG_FILES , DEFAULT_CONFIG , config from .plugin import plugin , PipenMainPlugin from .proc import Proc , ProcType from .progressbar import PipelinePBar from .exceptions import ProcDependencyError from .utils import ( get_console_width , get_plugin_context , log_rich_renderable , render_scope , logger , pipen_banner , DEFAULT_CONSOLE_WIDTH ) class Pipen : DOCS \"\"\"The Pipen class provides interface to assemble and run the pipeline\"\"\" PIPELINE_COUNT : ClassVar [ int ] = 0 def __init__ ( self , starts : Union [ ProcType , List [ ProcType ]], name : Optional [ str ] = None , desc : str = 'Undescribed.' , outdir : Optional [ PathLike ] = None , ** kwargs ) -> None : if Pipen . PIPELINE_COUNT == 0 : from . import __version__ PipenMainPlugin . __version__ = __version__ # make sure setup is called at runtime plugin . hooks . on_setup ( DEFAULT_CONFIG . plugin_opts ) config . _load ({ 'default' : DEFAULT_CONFIG }, * DEFAULT_CONFIG_FILES ) logger . setLevel ( config . loglevel . upper ()) log_rich_renderable ( pipen_banner (), 'magenta' , logger . info ) self . procs = None self . pbar = None self . name = name or f 'pipeline- { Pipen . PIPELINE_COUNT } ' self . desc = desc self . outdir = outdir or f './ { slugify ( self . name ) } -output' self . starts = [ starts ] if not isinstance ( starts , list ) else starts self . profile = 'default' self . _print_banner () self . config = config . copy () self . config . _load ({ 'default' : kwargs }) self . plugin_context = get_plugin_context ( self . config . plugins ) self . plugin_context . __enter__ () # make sure main plugin is enabled plugin . get_plugin ( 'main' ) . enable () logger . info ( 'Enabled plugins: %s ' , [ ' {name}{version} ' . format ( name = name , version = ( f '- { plg . __version__ } ' if plg . version else '' ) ) for name , plg in plugin . get_enabled_plugins () . items () ]) Pipen . PIPELINE_COUNT += 1 def _print_banner ( self ) -> None : \"\"\"Print he banner for the pipeline\"\"\" console_width = get_console_width () panel = Panel ( self . desc , title = self . name , box = box . HEAVY , width = min ( DEFAULT_CONSOLE_WIDTH , console_width )) logger . info ( '' ) log_rich_renderable ( panel , 'green' , logger . info ) def _print_config ( self ) -> None : \"\"\"Print the default configuration\"\"\" if self . profile == 'default' : context = self . config . _with ( 'default' ) else : context = self . config . _with ( self . profile , 'default' ) logger . info ( '' ) with context as conf : log_rich_renderable ( render_scope ( conf , 'default configurations' ), None , logger . info ) def _init ( self ) -> None : \"\"\"Initialize the pipeline\"\"\" max_proc_name_len = self . _init_procs ( self . starts ) desc_len = max ( len ( self . name ), max_proc_name_len ) self . pbar = PipelinePBar ( len ( self . procs ), self . name , desc_len ) # logger.debug('Calling hook: on_init ...') plugin . hooks . on_init ( self ) def _init_procs ( self , starts : List [ ProcType ]) -> int : \"\"\"Instantiate all processes Args: starts: The start processes Returns: The max length of all processes names (used to align the desc of the progressbar \"\"\" # logger.debug('Initializing all involved processes ...') self . procs = [ start if isinstance ( start , Proc ) else start () for start in starts ] max_proc_name_len = max ( len ( proc . name ) for proc in self . procs ) nexts = set ( sum (( proc . nexts for proc in self . procs ), [])) while nexts : # pick up one that can be added to procs for proc in nexts : if proc in self . procs : raise ProcDependencyError ( f 'Cyclic dependency: { proc . name } ' ) if not set ( proc . requires ) - set ( self . procs ): max_proc_name_len = max ( max_proc_name_len , len ( proc . name )) self . procs . append ( proc ) nexts . remove ( proc ) nexts |= set ( proc . nexts ) break else : if nexts : raise ProcDependencyError ( 'No available next process. ' 'Did you forget to start with some processes?' ) logger . info ( 'Loaded processes: %s ' , len ( self . procs )) return max_proc_name_len async def async_run ( self ) -> None : DOCS \"\"\"Run the processes one by one\"\"\" try : self . _init () logger . info ( 'Running pipeline using profile: %r ' , self . profile ) logger . info ( 'Output will be saved to: %r ' , str ( self . outdir )) self . _print_config () for proc in self . procs : self . pbar . update_proc_running () await proc . prepare ( self , self . profile ) await proc . run () if proc . succeeded : self . pbar . update_proc_done () else : self . pbar . update_proc_error () break proc . gc () # except Exception as exc: # logger.exception(exc) # sys.exit(1) # rich has a shift on line numbers finally : if self . pbar : self . pbar . done () self . plugin_context . __exit__ () def run ( self , profile : str = 'default' ) -> None : DOCS \"\"\"Run the pipeline with the given profile Args: profile: The default profile to use for the run Unless the profile is defined in the processes, otherwise this profile will be used \"\"\" self . profile = profile asyncio . run ( self . async_run ()) plugin . hooks . on_complete ( self )","title":"pipen.pipen"},{"location":"api/source/pipen.plugin/","text":"SOURCE CODE pipen. plugin DOCS \"\"\"Define hooks specifications and provide plugin manager\"\"\" import signal from pathlib import Path from typing import Any , Dict , Optional from xqute import JobStatus , Scheduler from xqute.utils import a_read_text , a_write_text from simplug import Simplug , SimplugResult from .defaults import ProcOutputType # pylint: disable=unused-argument,invalid-name plugin = Simplug ( 'pipen' ) @plugin . spec DOCS def on_setup ( plugin_config : Dict [ str , Any ]) -> None : \"\"\"Setup for plugins, primarily used for the plugins to setup some default configurations Args: plugin_config: The plugin configuration dictionary One should define a configuration item either with a prefix as the identity for the plugin or a namespace inside the plugin config. \"\"\" @plugin . spec DOCS def on_init ( pipen : \"Pipen\" ) -> None : \"\"\"The the pipeline is initialized. Args: pipen: The Pipen object \"\"\" @plugin . spec DOCS def on_complete ( pipen : \"Pipen\" ): \"\"\"The the pipeline is complete. Note that this hook is only called when the pipeline is successfully completed Args: pipen: The Pipen object \"\"\" @plugin . spec DOCS async def on_proc_init ( proc : \"Proc\" ): \"\"\"When a process is initialized Args: proc: The process \"\"\" @plugin . spec ( result = SimplugResult . FIRST ) DOCS def on_proc_shutdown ( proc : \"Proc\" , sig : Optional [ signal . Signals ]) -> None : \"\"\"When pipeline is shutting down, by Ctrl-c for example. Return False to stop shutting down, but you have to shut it down by yourself, for example, `proc.xqute.task.cancel()` Only the first return value will be used. Args: pipen: The xqute object sig: The signal. `None` means a natural shutdown \"\"\" @plugin . spec DOCS async def on_proc_done ( proc : \"Proc\" ): \"\"\"When a process is done This hook will be called anyway when a proc is succeeded or failed. To check if the process is succeeded, use `proc.succeeded` Args: proc: The process \"\"\" @plugin . spec DOCS async def on_job_init ( proc : \"Proc\" , job : \"Job\" ): \"\"\"When a job is initialized Args: proc: The process job: The job \"\"\" @plugin . spec DOCS async def on_job_queued ( proc : \"Proc\" , job : \"Job\" ): \"\"\"When a job is queued in xqute. Note it might not be queued yet in the scheduler system. Args: proc: The process job: The job \"\"\" @plugin . spec ( result = SimplugResult . FIRST ) DOCS async def on_job_submitting ( proc : \"Proc\" , job : \"Job\" ) -> Optional [ bool ]: \"\"\"When a job is submitting. The first plugin (based on priority) have this hook return False will cancel the submission Args: proc: The process job: The job Returns: False to cancel submission \"\"\" @plugin . spec DOCS async def on_job_submitted ( proc : \"Proc\" , job : \"Job\" ): \"\"\"When a job is submitted in the scheduler system. Args: proc: The process job: The job \"\"\" @plugin . spec DOCS async def on_job_running ( proc : \"Proc\" , job : \"Job\" ): \"\"\"When a job starts to run in scheduler system. Args: proc: The process job: The job \"\"\" @plugin . spec ( result = SimplugResult . FIRST ) DOCS async def on_job_killing ( proc : \"Proc\" , job : \"Job\" ) -> Optional [ bool ]: \"\"\"When a job is being killed. The first plugin (based on priority) have this hook return False will cancel the killing Args: proc: The process job: The job Returns: False to cancel killing \"\"\" @plugin . spec DOCS async def on_job_killed ( proc : \"Proc\" , job : \"Job\" ): \"\"\"When a job is killed Args: proc: The process job: The job \"\"\" @plugin . spec DOCS async def on_job_succeeded ( proc : \"Proc\" , job : \"Job\" ): \"\"\"When a job completes successfully. Args: proc: The process job: The job \"\"\" @plugin . spec DOCS async def on_job_failed ( proc : \"Proc\" , job : \"Job\" ): \"\"\"When a job is done but failed. Args: proc: The process job: The job \"\"\" plugin . load_entrypoints () class PipenMainPlugin : DOCS \"\"\"The builtin main plugin, used to update the progress bar and cache the job\"\"\" name = 'main' @plugin . impl def on_proc_shutdown ( self , proc : \"Proc\" , sig : Optional [ signal . Signals ]): \"\"\"When a process is shutting down\"\"\" if sig : proc . log ( 'warning' , 'Got signal %r , trying a graceful shutdown ...' , sig . name ) @plugin . impl async def on_job_submitted ( self , proc : \"Proc\" , job : \"Job\" ): \"\"\"Update the progress bar when a job is submitted\"\"\" proc . pbar . update_job_submitted () @plugin . impl async def on_job_running ( self , proc : \"Proc\" , job : \"Job\" ): \"\"\"Update the progress bar when a job starts to run\"\"\" proc . pbar . update_job_running () @plugin . impl async def on_job_succeeded ( self , proc : \"Proc\" , job : \"Job\" ): \"\"\"Cache the job and update the progress bar when a job is succeeded\"\"\" # now the returncode is 0, however, we need to check if output files # have been created or not, this makes sure job.cache not fail for outkey , outtype in job . _output_types . items (): if outtype == ProcOutputType . VAR : continue if not Path ( job . output [ outkey ]) . exists (): job . status = JobStatus . FAILED proc . pbar . update_job_failed () stderr = await a_read_text ( job . stderr_file ) stderr = ( f ' { stderr } \\n\\n Output { outtype } { outkey !r} ' 'is not generated.' ) await a_write_text ( job . stderr_file , stderr ) break else : await job . cache () proc . pbar . update_job_succeeded () @plugin . impl async def on_job_failed ( self , proc : \"Proc\" , job : \"Job\" ): \"\"\"Update the progress bar when a job is failed\"\"\" proc . pbar . update_job_failed () if job . status == JobStatus . RETRYING : job . log ( 'debug' , 'Retrying # %s ' , job . trial_count + 1 ) proc . pbar . update_job_retrying () plugin . register ( PipenMainPlugin ) xqute_plugin = Simplug ( 'xqute' ) class XqutePipenPlugin : DOCS \"\"\"The plugin for xqute working as proxy for pipen plugin hooks\"\"\" name = 'xqute.pipen' @xqute_plugin . impl def on_shutdown ( self , xqute : \"Xqute\" , sig : Optional [ signal . Signals ]): \"\"\"When a process is shutting down\"\"\" return plugin . hooks . on_proc_shutdown ( xqute . proc , sig ) @xqute_plugin . impl async def on_job_init ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is initialized\"\"\" await plugin . hooks . on_job_init ( job . proc , job ) @xqute_plugin . impl async def on_job_queued ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is queued\"\"\" await plugin . hooks . on_job_queued ( job . proc , job ) @xqute_plugin . impl async def on_job_submitting ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is being submitted\"\"\" return await plugin . hooks . on_job_submitting ( job . proc , job ) @xqute_plugin . impl async def on_job_submitted ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is submitted\"\"\" await plugin . hooks . on_job_submitted ( job . proc , job ) @xqute_plugin . impl async def on_job_running ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job starts to run\"\"\" await plugin . hooks . on_job_running ( job . proc , job ) @xqute_plugin . impl async def on_job_killing ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is being killed\"\"\" return await plugin . hooks . on_job_killing ( job . proc , job ) @xqute_plugin . impl async def on_job_killed ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is killed\"\"\" await plugin . hooks . on_job_killed ( job . proc , job ) @xqute_plugin . impl async def on_job_succeeded ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is succeeded\"\"\" await plugin . hooks . on_job_succeeded ( job . proc , job ) @xqute_plugin . impl async def on_job_failed ( self , scheduler : Scheduler , job : \"Job\" ): \"\"\"When a job is failed\"\"\" await plugin . hooks . on_job_failed ( job . proc , job ) xqute_plugin . register ( XqutePipenPlugin )","title":"pipen.plugin"},{"location":"api/source/pipen.proc/","text":"SOURCE CODE pipen. proc DOCS \"\"\"Provide the Proc class\"\"\" import asyncio import logging from pathlib import Path from typing import Any , ClassVar , Dict , Iterable , List , Optional , Type , Union from rich import box from rich.panel import Panel from slugify import slugify from varname import varname from simpleconf import Config from xqute import Xqute , JobStatus from xqute import Scheduler from pandas import DataFrame from .utils import ( brief_list , log_rich_renderable , logger , get_console_width , cached_property , DEFAULT_CONSOLE_WIDTH ) from .template import Template from .plugin import plugin from ._proc_properties import ProcProperties , ProcMeta , ProcType from .exceptions import ProcWorkdirConflictException class Proc ( ProcProperties , metaclass = ProcMeta ): DOCS \"\"\"The Proc class provides process assembly functionality\"\"\" name : ClassVar [ str ] = None desc : ClassVar [ str ] = None SELF : ClassVar [ \"Proc\" ] = None def __new__ ( cls , * args , ** kwargs ): DOCS \"\"\"Make sure cls() always get to the same instance\"\"\" if not args and not kwargs : if not cls . SELF or cls . SELF . __class__ is not cls : cls . SELF = super () . __new__ ( cls ) return cls . SELF return super () . __new__ ( cls ) # pylint: disable=redefined-builtin,redefined-outer-name def __init__ ( self , name : Optional [ str ] = None , desc : Optional [ str ] = None , * , end : Optional [ bool ] = None , input_keys : Union [ List [ str ], str ] = None , input : Optional [ Union [ str , Iterable [ str ]]] = None , output : Optional [ Union [ str , Iterable [ str ]]] = None , requires : Optional [ Union [ ProcType , Iterable [ ProcType ]]] = None , lang : Optional [ str ] = None , script : Optional [ str ] = None , forks : Optional [ int ] = None , cache : Optional [ bool ] = None , args : Optional [ Dict [ str , Any ]] = None , envs : Optional [ Dict [ str , Any ]] = None , dirsig : Optional [ bool ] = None , profile : Optional [ str ] = None , template : Optional [ Union [ str , Type [ Template ]]] = None , scheduler : Optional [ Union [ str , Scheduler ]] = None , scheduler_opts : Optional [ Dict [ str , Any ]] = None ) -> None : if getattr ( self , '_inited' , False ): return super () . __init__ ( end , input_keys , input , output , lang , script , forks , requires , args , envs , cache , dirsig , profile , template , scheduler , scheduler_opts ) self . nexts = [] self . name = ( name if name is not None else self . __class__ . name if self . __class__ . name is not None else self . __class__ . __name__ if self is self . __class__ . SELF else varname () ) self . desc = ( desc if desc is not None else self . __class__ . desc if self . __class__ . desc is not None else self . __doc__ . lstrip () . splitlines ()[ 0 ] if self . __doc__ else 'Undescribed.' ) self . pipeline = None self . pbar = None self . jobs = [] self . xqute = None self . workdir = None self . out_channel = None self . _inited = True def log ( self , DOCS level : Union [ int , str ], msg : str , * args , logger : logging . Logger = logger ) -> None : \"\"\"Log message for the process Args: level: The log level of the record msg: The message to log *args: The arguments to format the message logger: The logging logger \"\"\" msg = msg % args if not isinstance ( level , int ): level = logging . getLevelName ( level . upper ()) logger . log ( level , '[cyan] %s :[/cyan] %s ' , self . name , msg ) def gc ( self ): DOCS \"\"\"GC process for the process to save memory after it's done\"\"\" del self . xqute self . xqute = None del self . jobs [:] self . jobs = [] del self . pbar self . pbar = None async def prepare ( self , pipeline : \"Pipen\" , profile : str ) -> None : DOCS \"\"\"Prepare the process Args: pipeline: The Pipen object profile: The profile of the configuration \"\"\" if self . end is None and not self . nexts : self . end = True self . _print_banner () self . _print_dependencies () self . pipeline = pipeline profile = self . profile or profile if profile == 'default' : # no profile specified or profile is default, # we should use __init__ the highest priority config = pipeline . config . _use ( 'default' , copy = True ) else : config = pipeline . config . _use ( profile , 'default' , copy = True ) self . properties_from_config ( config ) self . compute_properties () self . workdir = Path ( config . workdir ) / slugify ( self . name ) # check if it's the same proc using the workdir proc_name_file = self . workdir / 'proc.name' if proc_name_file . is_file () and proc_name_file . read_text () != self . name : raise ProcWorkdirConflictException ( 'Workdir name is conflicting with process ' f ' { proc_name_file . read_text () !r} , use a differnt pipeline ' 'or a different process name.' ) self . workdir . mkdir ( parents = True , exist_ok = True ) proc_name_file . write_text ( self . name ) self . log ( 'info' , 'Workdir: %r ' , str ( self . workdir )) self . xqute = Xqute ( self . scheduler , job_metadir = self . workdir , job_submission_batch = config . submission_batch , job_error_strategy = config . error_strategy , job_num_retries = config . num_retries , scheduler_forks = self . forks , ** self . scheduler_opts ) # for the plugin hooks to access self . xqute . proc = self # init all other properties and jobs await self . _init_jobs ( config ) await plugin . hooks . on_proc_init ( self ) # init pbar self . pbar = pipeline . pbar . proc_bar ( self . size , self . name ) def __repr__ ( self ): return f '<Proc- { hex ( id ( self )) } ( { self . name } : { self . size } )>' @cached_property def size ( self ) -> int : \"\"\"The size of the process (# of jobs)\"\"\" return len ( self . jobs ) @cached_property def succeeded ( self ) -> bool : \"\"\"Check if the process is succeeded (all jobs succeeded)\"\"\" return all ( job . status == JobStatus . FINISHED for job in self . jobs ) async def run ( self ) -> None : DOCS \"\"\"Run the process\"\"\" cached_jobs = [] for job in self . jobs : if await job . cached : cached_jobs . append ( job . index ) self . pbar . update_job_submitted () self . pbar . update_job_running () self . pbar . update_job_succeeded () job . status = JobStatus . FINISHED await self . xqute . put ( job ) if cached_jobs : self . log ( 'info' , 'Cached jobs: %s ' , brief_list ( cached_jobs )) await self . xqute . run_until_complete () self . out_channel = DataFrame (( job . output for job in self . jobs )) self . pbar . done () await plugin . hooks . on_proc_done ( self ) async def _init_job ( self , worker_id : int , config : Config ) -> None : \"\"\"A worker to initialize jobs Args: worker_id: The worker id config: The pipeline configuration \"\"\" for job in self . jobs : if job . index % config . submission_batch != worker_id : continue await job . prepare ( self ) async def _init_jobs ( self , config : Config ) -> None : \"\"\"Initialize all jobs Args: config: The pipeline configuration \"\"\" for i in range ( self . input . data . shape [ 0 ]): job = self . scheduler . job_class ( i , '' , self . workdir ) self . jobs . append ( job ) await asyncio . gather ( * ( self . _init_job ( i , config ) for i in range ( config . submission_batch )) ) def _print_banner ( self ) -> None : \"\"\"Print the banner of the process\"\"\" console_width = get_console_width () panel = Panel ( self . desc , title = self . name , box = box . Box ( \"\u256d\u2550\u252c\u256e \\n \" \"\u2551 \u2551\u2551 \\n \" \"\u251c\u2550\u253c\u2524 \\n \" \"\u2551 \u2551\u2551 \\n \" \"\u251c\u2550\u253c\u2524 \\n \" \"\u251c\u2550\u253c\u2524 \\n \" \"\u2551 \u2551\u2551 \\n \" \"\u2570\u2550\u2534\u256f \\n \" ) if self . end else box . ROUNDED , width = min ( DEFAULT_CONSOLE_WIDTH , console_width ) ) logger . info ( '' ) log_rich_renderable ( panel , 'cyan' , logger . info ) def _print_dependencies ( self ): \"\"\"Print the dependencies\"\"\" if self . requires : self . log ( 'info' , '[yellow]<<<[/yellow] %s ' , [ proc . name for proc in self . requires ]) if self . nexts : self . log ( 'info' , '[yellow]>>>[/yellow] %s ' , [ proc . name for proc in self . nexts ])","title":"pipen.proc"},{"location":"api/source/pipen.progressbar/","text":"SOURCE CODE pipen. progressbar DOCS \"\"\"Provide the PipelinePBar and ProcPBar classes\"\"\" import enlighten class ProcPBar : DOCS \"\"\"The progress bar for processes\"\"\" def __init__ ( self , manager : enlighten . Manager , proc_size : int , proc_name : str ) -> None : self . submitted_counter = manager . counter ( total = proc_size , color = 'cyan' , desc = proc_name , unit = 'jobs' , leave = False ) self . running_counter = self . submitted_counter . add_subcounter ( 'yellow' ) self . success_counter = self . submitted_counter . add_subcounter ( 'green' ) self . failure_counter = self . submitted_counter . add_subcounter ( 'red' ) def update_job_submitted ( self ): DOCS \"\"\"Update the progress bar when a job is submitted\"\"\" self . submitted_counter . update () def update_job_retrying ( self ): DOCS \"\"\"Update the progress bar when a job is retrying\"\"\" # self.running_counter.count -= 1 self . failure_counter . update ( - 1 ) def update_job_running ( self ): DOCS \"\"\"Update the progress bar when a job is running\"\"\" self . running_counter . update_from ( self . submitted_counter ) def update_job_succeeded ( self ): DOCS \"\"\"Update the progress bar when a job is succeeded\"\"\" try : self . success_counter . update_from ( self . running_counter ) except ValueError : # pragma: no cover self . success_counter . update_from ( self . submitted_counter ) def update_job_failed ( self ): DOCS \"\"\"Update the progress bar when a job is failed\"\"\" try : self . failure_counter . update_from ( self . running_counter ) except ValueError : # pragma: no cover self . failure_counter . update_from ( self . submitted_counter ) def done ( self ): DOCS \"\"\"The process is done\"\"\" self . submitted_counter . close () class PipelinePBar : DOCS \"\"\"Progress bar for the pipeline\"\"\" def __init__ ( self , n_procs : int , ppln_name : str , desc_len : int ) -> None : self . manager = enlighten . get_manager () self . running_counter = self . manager . counter ( total = n_procs , color = 'yellow' , desc = f ' { ppln_name : < { desc_len }} :' , unit = 'procs' ) self . success_counter = self . running_counter . add_subcounter ( 'green' ) self . failure_counter = self . running_counter . add_subcounter ( 'red' ) self . desc_len = desc_len def proc_bar ( self , proc_size : int , proc_name : str ) -> ProcPBar : DOCS \"\"\"Get the progress bar for a process Args: proc_size: The size of the process proc_name: The name of the process Returns: The progress bar for the given process \"\"\" proc_name = f ' { proc_name : < { self . desc_len }} :' return ProcPBar ( self . manager , proc_size , proc_name ) def update_proc_running ( self ): DOCS \"\"\"Update the progress bar when a process is running\"\"\" self . running_counter . update () def update_proc_done ( self ): DOCS \"\"\"Update the progress bar when a process is done\"\"\" self . success_counter . update_from ( self . running_counter ) def update_proc_error ( self ): DOCS \"\"\"Update the progress bar when a process is errored\"\"\" self . failure_counter . update_from ( self . running_counter ) def done ( self ) -> None : DOCS \"\"\"When the pipeline is done\"\"\" self . running_counter . close () self . manager . stop ()","title":"pipen.progressbar"},{"location":"api/source/pipen.scheduler/","text":"SOURCE CODE pipen. scheduler DOCS \"\"\"Provide builting schedulers\"\"\" from typing import Type , Union from xqute import Scheduler from xqute.schedulers.local_scheduler import ( LocalJob as XquteLocalJob , LocalScheduler as XquteLocalScheduler ) from xqute.schedulers.sge_scheduler import ( SgeJob as XquteSgeJob , SgeScheduler as XquteSgeScheduler ) from .job import Job from .defaults import SCHEDULER_ENTRY_GROUP from .utils import load_entrypoints , is_subclass from .exceptions import NoSuchSchedulerError , WrongSchedulerTypeError class LocalJob ( Job ): DOCS \"\"\"Job class for local scheduler\"\"\" wrap_cmd = XquteLocalJob . wrap_cmd class LocalScheduler ( XquteLocalScheduler ): DOCS \"\"\"Local scheduler\"\"\" job_class = LocalJob class SgeJob ( Job ): DOCS \"\"\"Job class for sge scheduler\"\"\" wrap_cmd = XquteSgeJob . wrap_cmd class SgeScheduler ( XquteSgeScheduler ): DOCS \"\"\"Sge scheduler\"\"\" job_class = SgeJob def get_scheduler ( scheduler : Union [ str , Type [ Scheduler ]]) -> Type [ Scheduler ]: DOCS \"\"\"Get the scheduler by name of the scheduler class itself Args: scheduler: The scheduler class or name Returns: The scheduler class \"\"\" if is_subclass ( scheduler , Scheduler ): return scheduler if scheduler == 'local' : return LocalScheduler if scheduler == 'sge' : return SgeScheduler for name , obj in load_entrypoints ( SCHEDULER_ENTRY_GROUP ): # pragma: no cover if name == scheduler : if not is_subclass ( obj , Scheduler ): raise WrongSchedulerTypeError ( 'Scheduler should be a subclass of ' 'pipen.scheduler.Scheduler.' ) return obj raise NoSuchSchedulerError ( str ( scheduler ))","title":"pipen.scheduler"},{"location":"api/source/pipen.template/","text":"SOURCE CODE pipen. template DOCS \"\"\"Template adaptor for pipen\"\"\" from abc import ABC , abstractmethod from typing import Any , Dict , Optional , Type , Union from liquid import Liquid from .utils import load_entrypoints , is_subclass from .defaults import TEMPLATE_ENTRY_GROUP from .exceptions import NoSuchTemplateEngineError , WrongTemplateEnginTypeError __all__ = [ 'Template' , 'TemplateLiquid' , 'TemplateJinja2' , 'get_template_engine' ] class Template ( ABC ): DOCS \"\"\"Base class wrapper to wrap template for pipen\"\"\" def __init__ ( self , source : Any , ** envs ): # pylint: disable=unused-argument \"\"\"Template construct\"\"\" self . envs = envs def update_envs ( self , ** envs ): DOCS \"\"\"@API Register extra environment @params: **envs: The environment \"\"\" self . envs . update ( envs ) def render ( self , data : Optional [ Dict [ str , Any ]] = None ) -> str : DOCS \"\"\"@API Render the template @parmas: data (dict): The data used to render \"\"\" return self . _render ( data or {}) @abstractmethod def _render ( self , data : Dict [ str , Any ]) -> str : \"\"\"Implement rendering\"\"\" class TemplateLiquid ( Template ): DOCS \"\"\"Liquidpy template wrapper.\"\"\" name = 'liquid' def __init__ ( self , source : Any , ** envs ): \"\"\"Initiate the engine with source and envs Args: source: The souce text envs: The env data \"\"\" super () . __init__ ( source , ** envs ) self . engine = Liquid ( source , liquid_config = { 'strict' : False , 'mode' : 'python' }, ** self . envs ) def _render ( self , data : Dict [ str , Any ]) -> str : \"\"\"Render the template Args: data: The data used for rendering Returns The rendered string \"\"\" return self . engine . render ( ** data ) class TemplateJinja2 ( Template ): DOCS \"\"\"Jinja2 template wrapper\"\"\" name = 'jinja2' def __init__ ( self , source : Any , ** envs ): \"\"\"Initiate the engine with source and envs Args: source: The souce text envs: The env data \"\"\" import jinja2 super () . __init__ ( source , ** envs ) self . engine = jinja2 . Template ( source ) self . engine . globals = self . envs def _render ( self , data : Dict [ str , Any ]) -> str : \"\"\"Render the template Args: data: The data used for rendering Retuens: The rendered string \"\"\" return self . engine . render ( data ) def get_template_engine ( template : Union [ str , Type [ Template ]]) -> Type [ Template ]: DOCS \"\"\"Get the template engine by name or the template engine itself Args: template: The name of the template engine or the template engine itself Returns: The template engine \"\"\" if is_subclass ( template , Template ): return template if template == 'liquid' : return TemplateLiquid if template == 'jinja2' : return TemplateJinja2 for name , obj in load_entrypoints ( TEMPLATE_ENTRY_GROUP ): # pragma: no cover if name == template : if not is_subclass ( obj , Template ): raise WrongTemplateEnginTypeError ( 'Template engine should be a subclass of ' 'pipen.templates.Template.' ) return obj raise NoSuchTemplateEngineError ( str ( template ))","title":"pipen.template"},{"location":"api/source/pipen.utils/","text":"SOURCE CODE pipen. utils DOCS \"\"\"Provide some utilities\"\"\" import logging from os import PathLike from pathlib import Path from io import StringIO from typing import ( Any , Callable , Iterable , List , Mapping , Optional , Tuple , Union ) # pylint: disable=unused-import try : # pragma: no cover from functools import cached_property except ImportError : # pragma: no cover from cached_property import cached_property # pylint: enable=unused-import try : # pragma: no cover import importlib_metadata except ImportError : # pragma: no cover # pylint: disable=ungrouped-imports from importlib import metadata as importlib_metadata from rich.logging import RichHandler from rich.console import Console , RenderableType from rich.highlighter import ReprHighlighter from rich.table import Table from rich.text import Text from rich.panel import Panel from rich.pretty import Pretty from more_itertools import consecutive_groups from simplug import SimplugContext from .defaults import ( LOGGER_NAME , DEFAULT_CONSOLE_WIDTH , DEFAULT_CONSOLE_WIDTH_SHIFT ) from .exceptions import ConfigurationError from .plugin import plugin # pylint: disable=invalid-name _logger_handler = RichHandler ( show_path = False , show_level = False , rich_tracebacks = True , markup = True ) _logger_format = logging . Formatter ( '[logging.level. %(levelname)s ] %(levelname).1s [/logging.level. %(levelname)s ]' ' / %(plugin_name)-7s %(message)s ' , '%m- %d %H:%M:%S' ) _logger_handler . setFormatter ( _logger_format ) def get_logger ( name : str , DOCS level : Optional [ Union [ str , int ]] = None ) -> logging . Logger : \"\"\"Get the logger by given plugin name Args: level: The initial level of the logger Returns: The logger \"\"\" log = logging . getLogger ( f 'pipen. { name } ' ) log . addHandler ( _logger_handler ) if level is not None : log . setLevel ( level . upper () if isinstance ( level , str ) else level ) return logging . LoggerAdapter ( log , { 'plugin_name' : name }) logger = get_logger ( LOGGER_NAME ) def get_console_width ( default : int = DEFAULT_CONSOLE_WIDTH , DOCS shift : int = DEFAULT_CONSOLE_WIDTH_SHIFT ) -> int : \"\"\"Get the console width Args: default: The default console width if failed to get shift: The shift to subtract from the width as we have time, level, plugin name in log \"\"\" try : return logger . logger . handlers [ 0 ] . console . width - shift except ( AttributeError , IndexError ): # pragma: no cover return default - shift def get_plugin_context ( plugins : Optional [ List [ Any ]]) -> SimplugContext : DOCS \"\"\"Get the plugin context to enable and disable plugins per pipeline Args: plugins: A list of plugins to enable or a list of names with 'no:' as prefix to disable Returns: The plugin context manager \"\"\" if plugins is None : return plugin . plugins_only_context ( None ) no_plugins = [ isinstance ( plug , str ) and plug . startswith ( 'no:' ) for plug in plugins ] if any ( no_plugins ) and not all ( no_plugins ): raise ConfigurationError ( 'Either all plugin names start with \"no:\" or ' 'none of them does.' ) if all ( no_plugins ): return plugin . plugins_but_context ( plug [ 3 :] for plug in plugins ) return plugin . plugins_only_context ( plugins ) def log_rich_renderable ( DOCS renderable : RenderableType , color : str , logfunc : Callable , * args , ** kwargs ) -> None : \"\"\"Log a rich renderable to logger Args: renderable: The rich renderable splitline: Whether split the lines or log the entire message logfunc: The log function, if message is not the first argument, use functools.partial to wrap it *args: The arguments to the log function **kwargs: The keyword arguments to the log function \"\"\" console = Console ( file = StringIO ()) console . print ( renderable ) for line in console . file . getvalue () . splitlines (): logfunc ( f '[ { color } ] { line } [/ { color } ]' if color else line , * args , ** kwargs ) def render_scope ( scope : Mapping , title : str ) -> RenderableType : DOCS \"\"\"Log a mapping to console Args: scope: The mapping object title: The title of the scope \"\"\" highlighter = ReprHighlighter () items_table = Table . grid ( padding = ( 0 , 1 ), expand = False ) items_table . add_column ( justify = \"left\" ) for key , value in sorted ( scope . items ()): items_table . add_row ( Text . assemble (( key , \"scope.key\" )), Text . assemble (( '=' , 'scope.equals' )), Pretty ( value , highlighter = highlighter , overflow = 'fold' ) ) return Panel ( items_table , title = title , width = min ( DEFAULT_CONSOLE_WIDTH , get_console_width ()), border_style = \"scope.border\" , padding = ( 0 , 1 ), ) def pipen_banner () -> RenderableType : DOCS \"\"\"The banner for pipen\"\"\" from . import __version__ table = Table ( width = min ( DEFAULT_CONSOLE_WIDTH , get_console_width ()), show_header = False , show_edge = False , show_footer = False , show_lines = False , caption = f \"version: { __version__ } \" ) table . add_column ( justify = 'center' ) table . add_row ( r \" _____________________________________ __\" ) table . add_row ( r \" ___ __ \\___ _/__ __ \\__ ____/__ | / /\" ) table . add_row ( r \" __ /_/ /__ / __ /_/ /_ __/ __ |/ / \" ) table . add_row ( r \"_ ____/__/ / _ ____/_ /___ _ /| / \" ) table . add_row ( r \"/_/ /___/ /_/ /_____/ /_/ |_/ \" ) table . add_row ( \"\" ) return table def brief_list ( blist : List [ int ]) -> str : DOCS \"\"\"Briefly show an integer list, combine the continuous numbers. Args: blist: The list Returns: The string to show for the briefed list. \"\"\" ret = [] for group in consecutive_groups ( blist ): group = list ( group ) if len ( group ) > 1 : ret . append ( f ' { group [ 0 ] } - { group [ 1 ] } ' ) else : ret . append ( str ( group [ 0 ])) return ', ' . join ( ret ) def get_mtime ( path : PathLike , dir_depth : int = 1 ) -> float : DOCS \"\"\"Get the modification time of a path. If path is a directory, try to get the last modification time of the contents in the directory at given dir_depth Args: dir_depth: The depth of the directory to check the last modification time Returns: The last modification time of path \"\"\" path = Path ( path ) if not path . is_dir () or dir_depth == 0 : return path . stat () . st_mtime mtime = 0 for file in path . glob ( '*' ): mtime = max ( mtime , get_mtime ( file , dir_depth - 1 )) return mtime def is_subclass ( obj : Any , cls : type ) -> bool : DOCS \"\"\"Tell if obj is a subclass of cls Differences with issubclass is that we don't raise Type error if obj is not a class Args: obj: The object to check cls: The class to check Returns: True if obj is a subclass of cls otherwise False \"\"\" try : return issubclass ( obj , cls ) except TypeError : return False def load_entrypoints ( group : str ) -> Iterable [ Tuple [ str , Any ]]: DOCS \"\"\"Load objects from setuptools entrypoints by given group name Args: group: The group name of the entrypoints Returns: An iterable of tuples with name and the loaded object \"\"\" for dist in importlib_metadata . distributions (): # pragma: no cover for epoint in dist . entry_points : if epoint . group != group : continue obj = epoint . load () yield ( epoint . name , obj )","title":"pipen.utils"}]}